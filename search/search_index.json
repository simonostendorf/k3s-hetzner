{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"deployment/","text":"Deployment After the prevoius steps in this guide we got a working kubertenes cluster with a loadbalanced, high-available controlplane and communication between our local machine and the cluster. But we still miss some important features like agent-nodes, storage-drivers, ingress-controller and autoscalers. In this step we will setup all needed deployments for the cluster to work poperly. This step will not cover the deployment of the applications itself, but only the needed infrastructure .","title":"Deployment"},{"location":"deployment/#deployment","text":"After the prevoius steps in this guide we got a working kubertenes cluster with a loadbalanced, high-available controlplane and communication between our local machine and the cluster. But we still miss some important features like agent-nodes, storage-drivers, ingress-controller and autoscalers. In this step we will setup all needed deployments for the cluster to work poperly. This step will not cover the deployment of the applications itself, but only the needed infrastructure .","title":"Deployment"},{"location":"deployment/other/hpa/","text":"Example Horizontal Pod Autoscaler To show that the pod autoscaling and cluster autoscaling works we will deploy an example application. The example is based on the example of The DevOpy Guy . Example Application Download the example application and the traffic generator files into the deployments/hpa folder on your local machine: curl https://raw.githubusercontent.com/marcel-dempers/docker-development-youtube-series/master/kubernetes/autoscaling/components/application/deployment.yaml --create-dirs -L -o deployments/hpa/example-deployment-application.yml curl https://raw.githubusercontent.com/marcel-dempers/docker-development-youtube-series/master/kubernetes/autoscaling/components/application/traffic-generator.yaml --create-dirs -L -o deployments/hpa/example-deployment-traffic.yml Apply both deployments to the cluster: kubectl apply -f deployments/hpa/example-deployment-application.yml kubectl apply -f deployments/hpa/example-deployment-traffic.yml Execute the following commands to execute the traffic generator inside the traffic-container: kubectl exec -it traffic-generator -- sh apk add --no-cache wrk wrk -c 5 -t 5 -d 99999 -H \"Connection: Close\" http://application-cpu This command will install wrk,a modern HTTP benchmarking tool capable of generating load. The command will generate 5 concurrent connections with 5 threads for 99999 seconds. The load will be generated against the application-cpu service. With the following command you can see the top resource consuming pods: kubectl top pods Scale Pods To apply the horizontal autoscaling run the following command: kubectl autoscale deploy/application-cpu --cpu-percent = 95 --min = 1 --max = 10 This command will scale the application-cpu deployment to a maximum of 10 pods (and minimum of 1) if the cpu usage is higher than 95%. You can get information about the autoscaler with the following command: kubectl get hpa -owide See also scale up and down policies . Delete Deployment To delete the hpa and the example deployment run the following command: kubectl delete hpa application-cpu kubectl delete -f deployments/hpa/example-deployment-application.yml kubectl delete -f deployments/hpa/example-deployment-traffic.yml","title":"Horizontal Pod Autoscaler"},{"location":"deployment/other/hpa/#example-horizontal-pod-autoscaler","text":"To show that the pod autoscaling and cluster autoscaling works we will deploy an example application. The example is based on the example of The DevOpy Guy .","title":"Example Horizontal Pod Autoscaler"},{"location":"deployment/other/hpa/#example-application","text":"Download the example application and the traffic generator files into the deployments/hpa folder on your local machine: curl https://raw.githubusercontent.com/marcel-dempers/docker-development-youtube-series/master/kubernetes/autoscaling/components/application/deployment.yaml --create-dirs -L -o deployments/hpa/example-deployment-application.yml curl https://raw.githubusercontent.com/marcel-dempers/docker-development-youtube-series/master/kubernetes/autoscaling/components/application/traffic-generator.yaml --create-dirs -L -o deployments/hpa/example-deployment-traffic.yml Apply both deployments to the cluster: kubectl apply -f deployments/hpa/example-deployment-application.yml kubectl apply -f deployments/hpa/example-deployment-traffic.yml Execute the following commands to execute the traffic generator inside the traffic-container: kubectl exec -it traffic-generator -- sh apk add --no-cache wrk wrk -c 5 -t 5 -d 99999 -H \"Connection: Close\" http://application-cpu This command will install wrk,a modern HTTP benchmarking tool capable of generating load. The command will generate 5 concurrent connections with 5 threads for 99999 seconds. The load will be generated against the application-cpu service. With the following command you can see the top resource consuming pods: kubectl top pods","title":"Example Application"},{"location":"deployment/other/hpa/#scale-pods","text":"To apply the horizontal autoscaling run the following command: kubectl autoscale deploy/application-cpu --cpu-percent = 95 --min = 1 --max = 10 This command will scale the application-cpu deployment to a maximum of 10 pods (and minimum of 1) if the cpu usage is higher than 95%. You can get information about the autoscaler with the following command: kubectl get hpa -owide See also scale up and down policies .","title":"Scale Pods"},{"location":"deployment/other/hpa/#delete-deployment","text":"To delete the hpa and the example deployment run the following command: kubectl delete hpa application-cpu kubectl delete -f deployments/hpa/example-deployment-application.yml kubectl delete -f deployments/hpa/example-deployment-traffic.yml","title":"Delete Deployment"},{"location":"deployment/other/nginx/","text":"Example Nginx Deployment To show that the ingress controller, storage interface and cert-manager works, we will deploy an example nginx application. Create DNS record To make the ingress work, create a dns record at your dns-provider setup in an earlier step . You can name the subdomain whatever you want, i will keep it test in this scenario. Remember Remember to replace INGRESS_LOADBALANCER_IP with the public ip of your ingress loadbalancer (see hetzner cloud console) test.example.com -> A -> INGRESS_LOADBALANCER_IP Create deployment Create a new file on your machine with the following command: mkdir -p deployments/nginx nano deployments/nginx/example-deployment.yml Fill the file with the following content. Remember Remember to change the highlighted lines to fit your needs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : nginx-volumeclaim spec : storageClassName : hcloud-volumes accessModes : - ReadWriteOnce resources : requests : storage : 10Gi --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 progressDeadlineSeconds : 600 revisionHistoryLimit : 2 strategy : type : Recreate selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : volumes : - name : nginx-volume persistentVolumeClaim : claimName : nginx-volumeclaim containers : - name : nginx-container image : nginx:latest volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : nginx-volume --- apiVersion : v1 kind : Service metadata : name : nginx-service spec : selector : app : nginx ports : - name : nginx-port targetPort : 80 port : 80 --- apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : test-example-com spec : secretName : test-example-com-tls issuerRef : name : letsencrypt-production kind : ClusterIssuer commonName : \"test.example.com\" dnsNames : - \"test.example.com\" --- apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : nginx-ingressroute annotations : kubernetes.io/ingress.class : traefik-external spec : entryPoints : - websecure routes : - match : Host(`test.example.com`) kind : Rule services : - name : nginx-service port : 80 middlewares : - name : default-headers tls : secretName : test-example-com-tls The deployment contains the following parts: PersistentVolumeClaim: Volume claim of 5GB with driver hcloud-volumes (hetzner cloud volume) with access-mode ReadWriteOnce. See access-modes Deployment: Nginx container with 3 replicas and mounted volume Service: New service exposing port 80 from the nginx deployment Certificate: Certificate created for the test.example.com domain (or your domain) IngressRoute: Ingress route to tell traefik that the nginx-service (created in this deployment) should be available if the host matches test.example.com (or your domain) and should use the certificate created earlier Apply the deployment to the cluster: kubectl apply -f deployments/nginx/example-deployment.yml With the following command you can see the pods comming up: kubectl get pods Pick one of the pod-names and connect to this container with a new terminal: kubectl exec --stdin --tty CONTAINER_NAME -- /bin/bash Create the index page of the nginx page with the following command: echo 'Hello from k3s nginx.' > /usr/share/nginx/html/index.html exit Test Ingress and Certificate After you created the deployment, a certificate should be optained. You can follow the process by running: kubectl get certificates kubectl get challenges When the certificate is ready, you can test the ingress by opening the url https://test.example.com (or your custom url) in your browser. You should see the Hello from k3s nginx. page created in the step before. Delete Deployment To delete the deployment and all nested features (ingress-route, certificate, service etc.) run the following command: kubectl delete -f deployments/nginx/example-deployment.yml","title":"Example Nginx"},{"location":"deployment/other/nginx/#example-nginx-deployment","text":"To show that the ingress controller, storage interface and cert-manager works, we will deploy an example nginx application.","title":"Example Nginx Deployment"},{"location":"deployment/other/nginx/#create-dns-record","text":"To make the ingress work, create a dns record at your dns-provider setup in an earlier step . You can name the subdomain whatever you want, i will keep it test in this scenario. Remember Remember to replace INGRESS_LOADBALANCER_IP with the public ip of your ingress loadbalancer (see hetzner cloud console) test.example.com -> A -> INGRESS_LOADBALANCER_IP","title":"Create DNS record"},{"location":"deployment/other/nginx/#create-deployment","text":"Create a new file on your machine with the following command: mkdir -p deployments/nginx nano deployments/nginx/example-deployment.yml Fill the file with the following content. Remember Remember to change the highlighted lines to fit your needs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : nginx-volumeclaim spec : storageClassName : hcloud-volumes accessModes : - ReadWriteOnce resources : requests : storage : 10Gi --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 progressDeadlineSeconds : 600 revisionHistoryLimit : 2 strategy : type : Recreate selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : volumes : - name : nginx-volume persistentVolumeClaim : claimName : nginx-volumeclaim containers : - name : nginx-container image : nginx:latest volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : nginx-volume --- apiVersion : v1 kind : Service metadata : name : nginx-service spec : selector : app : nginx ports : - name : nginx-port targetPort : 80 port : 80 --- apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : test-example-com spec : secretName : test-example-com-tls issuerRef : name : letsencrypt-production kind : ClusterIssuer commonName : \"test.example.com\" dnsNames : - \"test.example.com\" --- apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : nginx-ingressroute annotations : kubernetes.io/ingress.class : traefik-external spec : entryPoints : - websecure routes : - match : Host(`test.example.com`) kind : Rule services : - name : nginx-service port : 80 middlewares : - name : default-headers tls : secretName : test-example-com-tls The deployment contains the following parts: PersistentVolumeClaim: Volume claim of 5GB with driver hcloud-volumes (hetzner cloud volume) with access-mode ReadWriteOnce. See access-modes Deployment: Nginx container with 3 replicas and mounted volume Service: New service exposing port 80 from the nginx deployment Certificate: Certificate created for the test.example.com domain (or your domain) IngressRoute: Ingress route to tell traefik that the nginx-service (created in this deployment) should be available if the host matches test.example.com (or your domain) and should use the certificate created earlier Apply the deployment to the cluster: kubectl apply -f deployments/nginx/example-deployment.yml With the following command you can see the pods comming up: kubectl get pods Pick one of the pod-names and connect to this container with a new terminal: kubectl exec --stdin --tty CONTAINER_NAME -- /bin/bash Create the index page of the nginx page with the following command: echo 'Hello from k3s nginx.' > /usr/share/nginx/html/index.html exit","title":"Create deployment"},{"location":"deployment/other/nginx/#test-ingress-and-certificate","text":"After you created the deployment, a certificate should be optained. You can follow the process by running: kubectl get certificates kubectl get challenges When the certificate is ready, you can test the ingress by opening the url https://test.example.com (or your custom url) in your browser. You should see the Hello from k3s nginx. page created in the step before.","title":"Test Ingress and Certificate"},{"location":"deployment/other/nginx/#delete-deployment","text":"To delete the deployment and all nested features (ingress-route, certificate, service etc.) run the following command: kubectl delete -f deployments/nginx/example-deployment.yml","title":"Delete Deployment"},{"location":"deployment/required/ccm/","text":"Cloud-Controller-Manager The first step is to deploy the cloud-controller-manager. This piece of software is needed to manage the cloud resources like loadbalancers, volumes and so on. This is the integration of the hetzner cloud api into the kubernets cluster. Setup Secret The first step is to create a kubernetes secret with our cloud api token that the cloud-controller-manager will use to authenticate against the hetzner cloud api. We have created the token in the preparation step . In my example configuration I have named the token cloud-controller-manager in the hetzner cloud. You also need the network-id from your private network. To get the id you can either copy the id from the hetzner cloud webinterface or copy the id from the following command: hcloud network list #(1)! The network id is the first column of the output. Create a new deployment file: mkdir -p deployments/ccm nano deployments/ccm/secret.yml And add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : hetzner-cloud-controller-manager namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! network : \"NETWORK_ID_HERE\" #(2)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cloud-controller-manager in this example. Replace NETWORK_ID_HERE with the network id you copied in the prerequisite step. Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/ccm/secret.yml Deploy CCM Download the latest version of the cloud controller manager deployment into the deployments/ccm folder on your local machine: curl https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/latest/download/ccm-networks.yaml --create-dirs -L -o deployments/ccm/deployment.yml Edit the deployment file and replace the secret name and the pod ip range. You can use the following commands to do this: sed -i 's/name: hcloud$/name: hetzner-cloud-controller-manager/' deployments/ccm/deployment.yml sed -i 's/10.244.0.0/10.100.0.0/' deployments/ccm/deployment.yml You can deploy the cloud controller manager with the following command from your local machine: kubectl apply -f deployments/ccm/deployment.yml After this step you should see pods comming up in the cluster. To validate the starting pods, run the following command: kubectl get pods -n kube-system #(1)! You have to use the kube-system namespace here, because the cloud-controller-manager is deployed in this namespace.","title":"Cloud Controller Manager"},{"location":"deployment/required/ccm/#cloud-controller-manager","text":"The first step is to deploy the cloud-controller-manager. This piece of software is needed to manage the cloud resources like loadbalancers, volumes and so on. This is the integration of the hetzner cloud api into the kubernets cluster.","title":"Cloud-Controller-Manager"},{"location":"deployment/required/ccm/#setup-secret","text":"The first step is to create a kubernetes secret with our cloud api token that the cloud-controller-manager will use to authenticate against the hetzner cloud api. We have created the token in the preparation step . In my example configuration I have named the token cloud-controller-manager in the hetzner cloud. You also need the network-id from your private network. To get the id you can either copy the id from the hetzner cloud webinterface or copy the id from the following command: hcloud network list #(1)! The network id is the first column of the output. Create a new deployment file: mkdir -p deployments/ccm nano deployments/ccm/secret.yml And add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : hetzner-cloud-controller-manager namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! network : \"NETWORK_ID_HERE\" #(2)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cloud-controller-manager in this example. Replace NETWORK_ID_HERE with the network id you copied in the prerequisite step. Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/ccm/secret.yml","title":"Setup Secret"},{"location":"deployment/required/ccm/#deploy-ccm","text":"Download the latest version of the cloud controller manager deployment into the deployments/ccm folder on your local machine: curl https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/latest/download/ccm-networks.yaml --create-dirs -L -o deployments/ccm/deployment.yml Edit the deployment file and replace the secret name and the pod ip range. You can use the following commands to do this: sed -i 's/name: hcloud$/name: hetzner-cloud-controller-manager/' deployments/ccm/deployment.yml sed -i 's/10.244.0.0/10.100.0.0/' deployments/ccm/deployment.yml You can deploy the cloud controller manager with the following command from your local machine: kubectl apply -f deployments/ccm/deployment.yml After this step you should see pods comming up in the cluster. To validate the starting pods, run the following command: kubectl get pods -n kube-system #(1)! You have to use the kube-system namespace here, because the cloud-controller-manager is deployed in this namespace.","title":"Deploy CCM"},{"location":"deployment/required/cert-manager/","text":"Cert-Manager We will use cert-manager as central certificate manager for all certificates in the cluster. You can find more about cert-manager on the official cert-manager website. Cert-Manager will use the letsencrypt service to issue certificates for the cluster. The certificates get validated through the dns01 acme challenge, described in the dns-provider step . Prerequisites Similar to traefik we will also use helm to install cert-manager to our cluster. You need the helm repository from cert-manager added to your local machine. You can add the repository with the following command: helm repo add jetstack https://charts.jetstack.io helm repo update And we also create a separate namespace for cert-manager with the following command: kubectl create namespace cert-manager Because kubernetes does not know about certificates in the default installation we need to create a custom resource definition for certificates. Run the following command to download and apply the custom resource definitions: curl https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.crds.yaml --create-dirs -L -o deployments/cert-manager/crds.yml kubectl apply -f deployments/cert-manager/crds.yml You can try to get information about certificates now with the following command: kubectl get certificates Before adding the crds kubernetes will return an error that the resource of the type certificates is not known. After adding the crds the command should return an empty list. Configure Helm Values Create a new helm values file for cert-manager with the following command: nano deployments/cert-manager/values.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 installCRDs : false replicaCount : 3 extraArgs : - --dns01-recursive-nameservers=1.1.1.1:53,9.9.9.9:53 - --dns01-recursive-nameservers-only podDnsPolicy : None podDnsConfig : nameservers : - \"1.1.1.1\" - \"9.9.9.9\" Edit the file contents if you want to change the dns-servers that are used to validate the dns01 challenge. In this example we will use cloudflare-dns (1.1.1.1) and quad9 (9.9.9.9). Deploy Workload To deploy the workload with helm run the following command on your local machine: helm install cert-manager jetstack/cert-manager --namespace cert-manager --values = deployments/cert-manager/values.yml --version v1.9.1 To see the pods comming up run the following command: kubectl get pods --namespace cert-manager Certificates To issue certificates you need different resources. The certificate-issuer (company that issues the certificate), the certificate-request (what certificate you want to issue) and the certificate (the actual certificate). In this example we will first use the letsencrypt staging issuer to issue test certificates for the domains we want to use and switch to the letsencrypt production environment if everything works. Create Cloudflare Token As described in the cloudflare step we've created a token for cloudflare to allow cert-manager to update the dns records. This token will be put into a kubernetes secret. Create a new kubernetes secret with the following command: nano deployments/cert-manager/cloudflare-secret.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : cloudflare-token-secret namespace : cert-manager type : Opaque stringData : cloudflare-token : CLOUDFLARE_TOKEN_HERE #(1)! Replace the CLOUDFLARE_TOKEN_HERE with the token you've created previously. Apply the secret to the cluster with the following command: kubectl apply -f deployments/cert-manager/cloudflare-secret.yml LetsEncrypt Staging As described previously we first use staging certificates to test our environment because the production api from letsencrypt is rate limited. Create CerificateIssuer Create a new certificate issuer with the following command: nano deployments/cert-manager/letsencrypt-staging-issuer.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : server : https://acme-staging-v02.api.letsencrypt.org/directory email : certificate@example.com #(1)! privateKeySecretRef : name : letsencrypt-staging solvers : - dns01 : cloudflare : email : cloudflare@example.com #(2)! apiTokenSecretRef : name : cloudflare-token-secret key : cloudflare-token selector : dnsZones : - \"example.com\" #(3)! certificate@example.com with your email address you want to use for letsencrypt cloudflare@example.com with your email address you use to login to cloudflare example.com with your zone name(s) inside cloudflare Apply the issuer to the cluster with the following command: kubectl apply -f deployments/cert-manager/letsencrypt-staging-issuer.yml Create Certificate The next step is to create a certificate. Create a new certificate with the following command: nano deployments/cert-manager/example-com-staging-tls.yml #(1)! Replace example-com-staging-tls.yml with the name of your certificate. Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : example-com #(1)! namespace : traefik spec : secretName : example-com-staging-tls #(2)! issuerRef : name : letsencrypt-staging kind : ClusterIssuer commonName : \"*.example.com\" #(3)! dnsNames : - \"example.com\" #(4)! - \"*.example.com\" #(5)! Replace example-com with the name of your certificate. Replace example-com-staging-tls with the name of your certificate. Replace *.example.com with the common name of your certificate. Replace example.com with the dns names you want the certificate for. Replace *.example.com with the dns names you want the certificate for. You can add multiple domains or use the certificate as wildcard certificate like its shown in the example. Attention The certificate will be created in the namespace trafik to add it to the traefik dashboard as test route. Certificates need to be in the same namespace as the IngressRoute. Apply the certificate to the cluster with the following command: kubectl apply -f deployments/cert-manager/example-com-staging-tls.yml You can see the certificates getting requested with the following commands: kubectl get challenges --namespace = traefik kubectl get certificates --namespace = traefik Add Certificate to traefik As final step you can add the certificate to the traefik dashboard. Reopen the traefik dashboard ingressroute and uncomment the tls section. nano deployments/traefik/dashboard-ingressroute.yml Edit the file and uncomment the tls section: 1 2 tls : secretName : example-com-staging-tls #(1)! Replace the example-com-staging-tls with your certificate name Apply the ingressroute to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Open the dashboard webpage and open the certificate details and check if the certificate is issued by letsencrypt. Remember We are using a staging (not valid) certificate, so dont worry if you get a warning in your browser. LetsEncrypt Production If everything works with the staging certificate we can switch to the production environment. You can delete the old staging certificate with the following commands: kubectl delete -f deployments/cert-manager/example-com-staging-tls.yml --namespace = traefik #(1)! Replace example-com-staging-tls.yml with the name of your certificate. Create CerificateIssuer The setup will be similar to the staging environment. Copy the staging issuer file: cp deployments/cert-manager/letsencrypt-staging-issuer.yml deployments/cert-manager/letsencrypt-production-issuer.yml sed -i 's/letsencrypt-staging/letsencrypt-production/g' deployments/cert-manager/letsencrypt-production-issuer.yml sed -i 's/-staging-/-/g' deployments/cert-manager/letsencrypt-production-issuer.yml Apply the issuer to the cluster with the following command: kubectl apply -f deployments/cert-manager/letsencrypt-production-issuer.yml Create Certificate Now we will create separate certificates for traefik and all other pods. In this example i will only show the creation of a production certificate for trafik but you can change the deployment file to fit your special needs. Create a new certificate with the following command: nano deployments/cert-manager/traefik-example-com-tls.yml #(1)! Replace traefik-example-com-tls.yml with the name of your certificate. Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : traefik-example-com #(1)! namespace : traefik spec : secretName : traefik-example-com-tls #(2)! issuerRef : name : letsencrypt-production kind : ClusterIssuer commonName : \"traefik.example.com\" #(3)! dnsNames : - \"traefik.example.com\" #(4)! Replace traefik-example-com with the name of your certificate. Replace traefik-example-com-tls with the name of your certificate. Replace traefik.example.com with the common name of your certificate. Replace traefik.example.com with the dns names you want the certificate for. Apply the certificate to the cluster with the following command: kubectl apply -f deployments/cert-manager/traefik-example-com-tls.yml #(1)! Replace traefik-example-com-tls.yml with the name of your certificate. You can see the certificates getting requested with the following commands: kubectl get challenges --namespace = traefik kubectl get certificates --namespace = traefik Add Certificate to traefik As final step you can add the certificate to the traefik dashboard. Reopen the traefik dashboard ingressroute and change the tls section. nano deployments/traefik/dashboard-ingressroute.yml Edit the file and change the tls section: 1 2 tls : secretName : traefik-example-com-tls #(1)! Replace the traefik-example-com-tls with your certificate name Apply the ingressroute to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Open the dashboard webpage and open the certificate details and check if the certificate is issued by letsencrypt and is a valid secure connection.","title":"Cert-Manager"},{"location":"deployment/required/cert-manager/#cert-manager","text":"We will use cert-manager as central certificate manager for all certificates in the cluster. You can find more about cert-manager on the official cert-manager website. Cert-Manager will use the letsencrypt service to issue certificates for the cluster. The certificates get validated through the dns01 acme challenge, described in the dns-provider step .","title":"Cert-Manager"},{"location":"deployment/required/cert-manager/#prerequisites","text":"Similar to traefik we will also use helm to install cert-manager to our cluster. You need the helm repository from cert-manager added to your local machine. You can add the repository with the following command: helm repo add jetstack https://charts.jetstack.io helm repo update And we also create a separate namespace for cert-manager with the following command: kubectl create namespace cert-manager Because kubernetes does not know about certificates in the default installation we need to create a custom resource definition for certificates. Run the following command to download and apply the custom resource definitions: curl https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.crds.yaml --create-dirs -L -o deployments/cert-manager/crds.yml kubectl apply -f deployments/cert-manager/crds.yml You can try to get information about certificates now with the following command: kubectl get certificates Before adding the crds kubernetes will return an error that the resource of the type certificates is not known. After adding the crds the command should return an empty list.","title":"Prerequisites"},{"location":"deployment/required/cert-manager/#configure-helm-values","text":"Create a new helm values file for cert-manager with the following command: nano deployments/cert-manager/values.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 installCRDs : false replicaCount : 3 extraArgs : - --dns01-recursive-nameservers=1.1.1.1:53,9.9.9.9:53 - --dns01-recursive-nameservers-only podDnsPolicy : None podDnsConfig : nameservers : - \"1.1.1.1\" - \"9.9.9.9\" Edit the file contents if you want to change the dns-servers that are used to validate the dns01 challenge. In this example we will use cloudflare-dns (1.1.1.1) and quad9 (9.9.9.9).","title":"Configure Helm Values"},{"location":"deployment/required/cert-manager/#deploy-workload","text":"To deploy the workload with helm run the following command on your local machine: helm install cert-manager jetstack/cert-manager --namespace cert-manager --values = deployments/cert-manager/values.yml --version v1.9.1 To see the pods comming up run the following command: kubectl get pods --namespace cert-manager","title":"Deploy Workload"},{"location":"deployment/required/cert-manager/#certificates","text":"To issue certificates you need different resources. The certificate-issuer (company that issues the certificate), the certificate-request (what certificate you want to issue) and the certificate (the actual certificate). In this example we will first use the letsencrypt staging issuer to issue test certificates for the domains we want to use and switch to the letsencrypt production environment if everything works.","title":"Certificates"},{"location":"deployment/required/cert-manager/#create-cloudflare-token","text":"As described in the cloudflare step we've created a token for cloudflare to allow cert-manager to update the dns records. This token will be put into a kubernetes secret. Create a new kubernetes secret with the following command: nano deployments/cert-manager/cloudflare-secret.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : cloudflare-token-secret namespace : cert-manager type : Opaque stringData : cloudflare-token : CLOUDFLARE_TOKEN_HERE #(1)! Replace the CLOUDFLARE_TOKEN_HERE with the token you've created previously. Apply the secret to the cluster with the following command: kubectl apply -f deployments/cert-manager/cloudflare-secret.yml","title":"Create Cloudflare Token"},{"location":"deployment/required/cert-manager/#letsencrypt-staging","text":"As described previously we first use staging certificates to test our environment because the production api from letsencrypt is rate limited.","title":"LetsEncrypt Staging"},{"location":"deployment/required/cert-manager/#create-cerificateissuer","text":"Create a new certificate issuer with the following command: nano deployments/cert-manager/letsencrypt-staging-issuer.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : server : https://acme-staging-v02.api.letsencrypt.org/directory email : certificate@example.com #(1)! privateKeySecretRef : name : letsencrypt-staging solvers : - dns01 : cloudflare : email : cloudflare@example.com #(2)! apiTokenSecretRef : name : cloudflare-token-secret key : cloudflare-token selector : dnsZones : - \"example.com\" #(3)! certificate@example.com with your email address you want to use for letsencrypt cloudflare@example.com with your email address you use to login to cloudflare example.com with your zone name(s) inside cloudflare Apply the issuer to the cluster with the following command: kubectl apply -f deployments/cert-manager/letsencrypt-staging-issuer.yml","title":"Create CerificateIssuer"},{"location":"deployment/required/cert-manager/#create-certificate","text":"The next step is to create a certificate. Create a new certificate with the following command: nano deployments/cert-manager/example-com-staging-tls.yml #(1)! Replace example-com-staging-tls.yml with the name of your certificate. Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : example-com #(1)! namespace : traefik spec : secretName : example-com-staging-tls #(2)! issuerRef : name : letsencrypt-staging kind : ClusterIssuer commonName : \"*.example.com\" #(3)! dnsNames : - \"example.com\" #(4)! - \"*.example.com\" #(5)! Replace example-com with the name of your certificate. Replace example-com-staging-tls with the name of your certificate. Replace *.example.com with the common name of your certificate. Replace example.com with the dns names you want the certificate for. Replace *.example.com with the dns names you want the certificate for. You can add multiple domains or use the certificate as wildcard certificate like its shown in the example. Attention The certificate will be created in the namespace trafik to add it to the traefik dashboard as test route. Certificates need to be in the same namespace as the IngressRoute. Apply the certificate to the cluster with the following command: kubectl apply -f deployments/cert-manager/example-com-staging-tls.yml You can see the certificates getting requested with the following commands: kubectl get challenges --namespace = traefik kubectl get certificates --namespace = traefik","title":"Create Certificate"},{"location":"deployment/required/cert-manager/#add-certificate-to-traefik","text":"As final step you can add the certificate to the traefik dashboard. Reopen the traefik dashboard ingressroute and uncomment the tls section. nano deployments/traefik/dashboard-ingressroute.yml Edit the file and uncomment the tls section: 1 2 tls : secretName : example-com-staging-tls #(1)! Replace the example-com-staging-tls with your certificate name Apply the ingressroute to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Open the dashboard webpage and open the certificate details and check if the certificate is issued by letsencrypt. Remember We are using a staging (not valid) certificate, so dont worry if you get a warning in your browser.","title":"Add Certificate to traefik"},{"location":"deployment/required/cert-manager/#letsencrypt-production","text":"If everything works with the staging certificate we can switch to the production environment. You can delete the old staging certificate with the following commands: kubectl delete -f deployments/cert-manager/example-com-staging-tls.yml --namespace = traefik #(1)! Replace example-com-staging-tls.yml with the name of your certificate.","title":"LetsEncrypt Production"},{"location":"deployment/required/cert-manager/#create-cerificateissuer_1","text":"The setup will be similar to the staging environment. Copy the staging issuer file: cp deployments/cert-manager/letsencrypt-staging-issuer.yml deployments/cert-manager/letsencrypt-production-issuer.yml sed -i 's/letsencrypt-staging/letsencrypt-production/g' deployments/cert-manager/letsencrypt-production-issuer.yml sed -i 's/-staging-/-/g' deployments/cert-manager/letsencrypt-production-issuer.yml Apply the issuer to the cluster with the following command: kubectl apply -f deployments/cert-manager/letsencrypt-production-issuer.yml","title":"Create CerificateIssuer"},{"location":"deployment/required/cert-manager/#create-certificate_1","text":"Now we will create separate certificates for traefik and all other pods. In this example i will only show the creation of a production certificate for trafik but you can change the deployment file to fit your special needs. Create a new certificate with the following command: nano deployments/cert-manager/traefik-example-com-tls.yml #(1)! Replace traefik-example-com-tls.yml with the name of your certificate. Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : traefik-example-com #(1)! namespace : traefik spec : secretName : traefik-example-com-tls #(2)! issuerRef : name : letsencrypt-production kind : ClusterIssuer commonName : \"traefik.example.com\" #(3)! dnsNames : - \"traefik.example.com\" #(4)! Replace traefik-example-com with the name of your certificate. Replace traefik-example-com-tls with the name of your certificate. Replace traefik.example.com with the common name of your certificate. Replace traefik.example.com with the dns names you want the certificate for. Apply the certificate to the cluster with the following command: kubectl apply -f deployments/cert-manager/traefik-example-com-tls.yml #(1)! Replace traefik-example-com-tls.yml with the name of your certificate. You can see the certificates getting requested with the following commands: kubectl get challenges --namespace = traefik kubectl get certificates --namespace = traefik","title":"Create Certificate"},{"location":"deployment/required/cert-manager/#add-certificate-to-traefik_1","text":"As final step you can add the certificate to the traefik dashboard. Reopen the traefik dashboard ingressroute and change the tls section. nano deployments/traefik/dashboard-ingressroute.yml Edit the file and change the tls section: 1 2 tls : secretName : traefik-example-com-tls #(1)! Replace the traefik-example-com-tls with your certificate name Apply the ingressroute to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Open the dashboard webpage and open the certificate details and check if the certificate is issued by letsencrypt and is a valid secure connection.","title":"Add Certificate to traefik"},{"location":"deployment/required/cluster-autoscaler/","text":"Cluster-Autoscaler In the whole setup we didnt setup agent (worker) nodes for the cluster. This is because we want to use the cluster-autoscaler to scale the cluster up and down based on the current workload. In this step we will create the configuration for our agent nodes and the corresponding deployment for the cluster-autoscaler. Create cloud-init Configuration The cluster-autoscaler will create new vms inside the hetzner cloud. To configure the new vms we need to create a cloud-init configuration. The cloud-init configuration will install needed packages, do needed configuration, install k3s and join the cluster. Create a new file for the cloud-init configuration and edit the file contents: mkdir -p deployments/cluster-autoscaler nano deployments/cluster-autoscaler/cloud-init.yml Create the following cloud-init configuration: Replace values Please replace YOUR_TIMEZONE with your timezone used for the servers before. You also have to replace YOUR_HETZNER_TOKEN with your hetzner token. In this example the token is named cluster-autoscaler Also replace YOUR_NETWORK_ID with your network id of the private network. You looked up the network id in a previous step but you can also look it up with hcloud network list . Replace YOUR_K3S_TOKEN with your k3s token, created in the k3s-setup step . 1 2 3 4 5 6 7 8 9 10 11 #cloud-config runcmd : - apt update - apt upgrade -y - apt install apparmor apparmor-utils python3-pip -y - timedatectl set-timezone YOUR_TIMEZONE - pip install hcloud - curl https://raw.githubusercontent.com/simonostendorf/k3s-hetzner/main/scripts/setup-agent-nodes.py -L -o setup-agent-nodes.py - python3 setup-agent-nodes.py --token YOUR_HETZNER_TOKEN --server_name $(hostname -f) --network_id YOUR_NETWORK_ID - sleep 20 - curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=\"v1.25.0-rc1+k3s1\" K3S_TOKEN=\"YOUR_K3S_TOKEN\" K3S_URL=\"https://10.0.0.100:6443\" INSTALL_K3S_EXEC=\"agent --node-name=\"$(hostname -f)\" --kubelet-arg=\"cloud-provider=external\" --node-ip=$(hostname -I | awk '{print $2}') --node-external-ip=$(hostname -I | awk '{print $1}') --flannel-iface=ens10\" sh - The cluster autoscaler needs the cloud-init configuration as base64 encoded string. You can encode the file with the following command: openssl enc -base64 -in deployments/cluster-autoscaler/cloud-init.yml -out deployments/cluster-autoscaler/cloud-init.yml.b64 Create Secret You have to create a new secret for the cluster autoscaler. The secret will contain the hetzner cloud token that the cluster autoscaler can create servers inside the hetzner cloud. The secret will also contain the base64 encoded cloud-init configuration. Create a new deployment file: mkdir -p deployments/cluster-autoscaler nano deployments/cluster-autoscaler/secret.yml And add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : hetzner-cluster-autoscaler namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! cloud-init : \"CLOUD_INIT_HERE\" #(2)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cluster-autoscaler in this example. Replace CLOUD_INIT_HERE with the base64 encoded cloud-init configuration. You can read the file with cat deployments/cluster-autoscaler/cloud-init.yml.b64 and copy the content. You can print your base64 encoded cloud-init configuration without newlines with the following command: cat deployments/cluster-autoscaler/cloud-init.yml.b64 | awk '{ printf(\"%s\", $0) }' Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/cluster-autoscaler/secret.yml Create autoscaler Image As described in the preparation step we need to create a custom image for the cluster-autoscaler using go. Clone the autoscaler git repository into a new folder using the following command: git clone https://github.com/kubernetes/autoscaler cd autoscaler/cluster-autoscaler Start the build process with the following commands: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . make build-in-docker docker build -t DOCKER_USERNAME/k8s-cluster-autoscaler:latest -f Dockerfile.amd64 . Push the created docker-image to your docker registry with the following command: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . docker push DOCKER_USERNAME/k8s-cluster-autoscaler:latest Go back to your old working directory with the following command: cd ../.. Create Registry Secret To pull the custom image from the docker registry we need to create a secret inside the cluster to get access to the container registry. You can create the secret from the commandline with the following command: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . You have to replace DOCKER_TOKEN with your docker token, created in the prerequisite step . Be shure to choose the read-only token (named k8s-hetzner in this example) kubectl create secret docker-registry -n kube-system dockerhub --docker-server = docker.io --docker-username = DOCKER_USERNAME --docker-password = DOCKER_TOKEN Configure Deployment To deploy the cluster autoscaler you have to create a deployment file. You can download the latest deployment file with the following command: curl https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/hetzner/examples/cluster-autoscaler-run-on-master.yaml --create-dirs -L -o deployments/cluster-autoscaler/deployment.yml Run the following command to replace the node tolerations: sed -z --in-place \"s# - effect: NoSchedule\\n key: node-role.kubernetes.io/master# - key: CriticalAddonsOnly\\n operator: Exists#g\" deployments/cluster-autoscaler/deployment.yml Edit the file with the following command: nano deployments/cluster-autoscaler/deployment.yml Run the following commands to replace the values in the deployment file: Remember Remember to replace DOCKER_USERNAME with your docker username created in the prerequisite step . Remember to configure the node pools and timeouts to fit your needs. sed -z --in-place \"s|- image: k8s.gcr.io/autoscaling/cluster-autoscaler:latest # or your custom image|- image: DOCKER_USERNAME/k8s-cluster-autoscaler:latest|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|- --nodes=1:10:CPX11:FSN1:pool1|- --nodes=1:10:CX21:HEL1:k8s-agent-hel1\\n - --nodes=1:10:CX21:FSN1:k8s-agent-fsn1\\n - --nodes=1:10:CX21:NBG1:k8s-agent-nbg1\\n - --scale-down-delay-after-add=10m0s\\n - --scale-down-unneeded-time=10m0s\\n - --scale-down-unready-time=5m0s\\n - --balance-similar-node-groups=true|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|secretKeyRef:\\n name: hcloud|secretKeyRef:\\n name: hetzner-cluster-autoscaler|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|HCLOUD_CLOUD_INIT\\n value: <your-cloud-init-data-base64-encoded>|HCLOUD_CLOUD_INIT\\n valueFrom:\\n secretKeyRef:\\n name: hetzner-cluster-autoscaler\\n key: cloud-init\\n - name: HCLOUD_IMAGE\\n value: debian-11|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|name: gitlab-registry|name: dockerhub|g\" deployments/cluster-autoscaler/deployment.yml Attention Do not set the environment variable HCLOUD_PUBLIC_IPV4 ! The hetzner cloud-api is only available via IPv4. The nodes have to reach the cloud-api because they run a python-script at boot to assign the correct private ip. You can find more information about this in the hetzner github issue . ToDo Maybe HCLOUD_PLACEMENT_GROUP is a possible option, but its not tested yet. The default configuration will create 3 agent pools with minimal 1 node and maximal 10 nodes. The nodes will be created with the CX21 server type and will be located in the FSN1 / HEL1 and NBG1 datacenter. Information The minimal nodes will not be created directly. The cluster-autoscaler needs a scaleup-event to create new nodes. The agents will get created if they are needed and then not deleted if you define a minimum node number. Deploy Workload You can apply the cluster autoscaler with the following command: kubectl apply -f deployments/cluster-autoscaler/deployment.yml","title":"Cluster-Autoscaler"},{"location":"deployment/required/cluster-autoscaler/#cluster-autoscaler","text":"In the whole setup we didnt setup agent (worker) nodes for the cluster. This is because we want to use the cluster-autoscaler to scale the cluster up and down based on the current workload. In this step we will create the configuration for our agent nodes and the corresponding deployment for the cluster-autoscaler.","title":"Cluster-Autoscaler"},{"location":"deployment/required/cluster-autoscaler/#create-cloud-init-configuration","text":"The cluster-autoscaler will create new vms inside the hetzner cloud. To configure the new vms we need to create a cloud-init configuration. The cloud-init configuration will install needed packages, do needed configuration, install k3s and join the cluster. Create a new file for the cloud-init configuration and edit the file contents: mkdir -p deployments/cluster-autoscaler nano deployments/cluster-autoscaler/cloud-init.yml Create the following cloud-init configuration: Replace values Please replace YOUR_TIMEZONE with your timezone used for the servers before. You also have to replace YOUR_HETZNER_TOKEN with your hetzner token. In this example the token is named cluster-autoscaler Also replace YOUR_NETWORK_ID with your network id of the private network. You looked up the network id in a previous step but you can also look it up with hcloud network list . Replace YOUR_K3S_TOKEN with your k3s token, created in the k3s-setup step . 1 2 3 4 5 6 7 8 9 10 11 #cloud-config runcmd : - apt update - apt upgrade -y - apt install apparmor apparmor-utils python3-pip -y - timedatectl set-timezone YOUR_TIMEZONE - pip install hcloud - curl https://raw.githubusercontent.com/simonostendorf/k3s-hetzner/main/scripts/setup-agent-nodes.py -L -o setup-agent-nodes.py - python3 setup-agent-nodes.py --token YOUR_HETZNER_TOKEN --server_name $(hostname -f) --network_id YOUR_NETWORK_ID - sleep 20 - curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=\"v1.25.0-rc1+k3s1\" K3S_TOKEN=\"YOUR_K3S_TOKEN\" K3S_URL=\"https://10.0.0.100:6443\" INSTALL_K3S_EXEC=\"agent --node-name=\"$(hostname -f)\" --kubelet-arg=\"cloud-provider=external\" --node-ip=$(hostname -I | awk '{print $2}') --node-external-ip=$(hostname -I | awk '{print $1}') --flannel-iface=ens10\" sh - The cluster autoscaler needs the cloud-init configuration as base64 encoded string. You can encode the file with the following command: openssl enc -base64 -in deployments/cluster-autoscaler/cloud-init.yml -out deployments/cluster-autoscaler/cloud-init.yml.b64","title":"Create cloud-init Configuration"},{"location":"deployment/required/cluster-autoscaler/#create-secret","text":"You have to create a new secret for the cluster autoscaler. The secret will contain the hetzner cloud token that the cluster autoscaler can create servers inside the hetzner cloud. The secret will also contain the base64 encoded cloud-init configuration. Create a new deployment file: mkdir -p deployments/cluster-autoscaler nano deployments/cluster-autoscaler/secret.yml And add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : hetzner-cluster-autoscaler namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! cloud-init : \"CLOUD_INIT_HERE\" #(2)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cluster-autoscaler in this example. Replace CLOUD_INIT_HERE with the base64 encoded cloud-init configuration. You can read the file with cat deployments/cluster-autoscaler/cloud-init.yml.b64 and copy the content. You can print your base64 encoded cloud-init configuration without newlines with the following command: cat deployments/cluster-autoscaler/cloud-init.yml.b64 | awk '{ printf(\"%s\", $0) }' Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/cluster-autoscaler/secret.yml","title":"Create Secret"},{"location":"deployment/required/cluster-autoscaler/#create-autoscaler-image","text":"As described in the preparation step we need to create a custom image for the cluster-autoscaler using go. Clone the autoscaler git repository into a new folder using the following command: git clone https://github.com/kubernetes/autoscaler cd autoscaler/cluster-autoscaler Start the build process with the following commands: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . make build-in-docker docker build -t DOCKER_USERNAME/k8s-cluster-autoscaler:latest -f Dockerfile.amd64 . Push the created docker-image to your docker registry with the following command: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . docker push DOCKER_USERNAME/k8s-cluster-autoscaler:latest Go back to your old working directory with the following command: cd ../..","title":"Create autoscaler Image"},{"location":"deployment/required/cluster-autoscaler/#create-registry-secret","text":"To pull the custom image from the docker registry we need to create a secret inside the cluster to get access to the container registry. You can create the secret from the commandline with the following command: Replace values You have to replace DOCKER_USERNAME with your docker username, created in the prerequisite step . You have to replace DOCKER_TOKEN with your docker token, created in the prerequisite step . Be shure to choose the read-only token (named k8s-hetzner in this example) kubectl create secret docker-registry -n kube-system dockerhub --docker-server = docker.io --docker-username = DOCKER_USERNAME --docker-password = DOCKER_TOKEN","title":"Create Registry Secret"},{"location":"deployment/required/cluster-autoscaler/#configure-deployment","text":"To deploy the cluster autoscaler you have to create a deployment file. You can download the latest deployment file with the following command: curl https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/hetzner/examples/cluster-autoscaler-run-on-master.yaml --create-dirs -L -o deployments/cluster-autoscaler/deployment.yml Run the following command to replace the node tolerations: sed -z --in-place \"s# - effect: NoSchedule\\n key: node-role.kubernetes.io/master# - key: CriticalAddonsOnly\\n operator: Exists#g\" deployments/cluster-autoscaler/deployment.yml Edit the file with the following command: nano deployments/cluster-autoscaler/deployment.yml Run the following commands to replace the values in the deployment file: Remember Remember to replace DOCKER_USERNAME with your docker username created in the prerequisite step . Remember to configure the node pools and timeouts to fit your needs. sed -z --in-place \"s|- image: k8s.gcr.io/autoscaling/cluster-autoscaler:latest # or your custom image|- image: DOCKER_USERNAME/k8s-cluster-autoscaler:latest|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|- --nodes=1:10:CPX11:FSN1:pool1|- --nodes=1:10:CX21:HEL1:k8s-agent-hel1\\n - --nodes=1:10:CX21:FSN1:k8s-agent-fsn1\\n - --nodes=1:10:CX21:NBG1:k8s-agent-nbg1\\n - --scale-down-delay-after-add=10m0s\\n - --scale-down-unneeded-time=10m0s\\n - --scale-down-unready-time=5m0s\\n - --balance-similar-node-groups=true|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|secretKeyRef:\\n name: hcloud|secretKeyRef:\\n name: hetzner-cluster-autoscaler|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|HCLOUD_CLOUD_INIT\\n value: <your-cloud-init-data-base64-encoded>|HCLOUD_CLOUD_INIT\\n valueFrom:\\n secretKeyRef:\\n name: hetzner-cluster-autoscaler\\n key: cloud-init\\n - name: HCLOUD_IMAGE\\n value: debian-11|g\" deployments/cluster-autoscaler/deployment.yml sed -z --in-place \"s|name: gitlab-registry|name: dockerhub|g\" deployments/cluster-autoscaler/deployment.yml Attention Do not set the environment variable HCLOUD_PUBLIC_IPV4 ! The hetzner cloud-api is only available via IPv4. The nodes have to reach the cloud-api because they run a python-script at boot to assign the correct private ip. You can find more information about this in the hetzner github issue . ToDo Maybe HCLOUD_PLACEMENT_GROUP is a possible option, but its not tested yet. The default configuration will create 3 agent pools with minimal 1 node and maximal 10 nodes. The nodes will be created with the CX21 server type and will be located in the FSN1 / HEL1 and NBG1 datacenter. Information The minimal nodes will not be created directly. The cluster-autoscaler needs a scaleup-event to create new nodes. The agents will get created if they are needed and then not deleted if you define a minimum node number.","title":"Configure Deployment"},{"location":"deployment/required/cluster-autoscaler/#deploy-workload","text":"You can apply the cluster autoscaler with the following command: kubectl apply -f deployments/cluster-autoscaler/deployment.yml","title":"Deploy Workload"},{"location":"deployment/required/csi/","text":"Cloud Storage Interface To use hetzner cloud volumes as persistent volume claims in kubernetes, we need to deploy the cloud-volume driver. The driver will than handle the volume claims and create the volumes in hetzner cloud. You can find more about the driver on the official hetzner-csi github repository. Setup Secret Similar to the cloud-controller-manager in a previous step, we have to create a secret for the cloud-storage-interface. We have created the token in the preparation step . In my example configuration I have named the token cloud-storage-interface in the hetzner cloud. Create a new deployment file: mkdir -p deployments/csi nano deployments/csi/secret.yml And add the following content: 1 2 3 4 5 6 7 apiVersion : v1 kind : Secret metadata : name : hetzner-container-storage-interface namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cloud-storage-interface in this example. Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/csi/secret.yml Deploy hcloud-csi Download the latest version of the storage driver deployment into the deployments/csi folder on your local machine: curl https://raw.githubusercontent.com/hetznercloud/csi-driver/v1.6.0/deploy/kubernetes/hcloud-csi.yml --create-dirs -L -o deployments/csi/deployment.yml Edit the deployment file and replace the secret name. You can use the following command to do this: sed -i 's/^.\\{18\\}name: hcloud-csi$/ name: hetzner-container-storage-interface/' deployments/csi/deployment.yml You can deploy the storage-interface with the following command from your local machine: kubectl apply -f deployments/csi/deployment.yml After this step you should see pods comming up in the cluster. To validate the starting pods, run the following command: kubectl get pods -n kube-system #(1)! You have to use the kube-system namespace here, because the storage-interface is deployed in this namespace.","title":"Cloud Storage Interface"},{"location":"deployment/required/csi/#cloud-storage-interface","text":"To use hetzner cloud volumes as persistent volume claims in kubernetes, we need to deploy the cloud-volume driver. The driver will than handle the volume claims and create the volumes in hetzner cloud. You can find more about the driver on the official hetzner-csi github repository.","title":"Cloud Storage Interface"},{"location":"deployment/required/csi/#setup-secret","text":"Similar to the cloud-controller-manager in a previous step, we have to create a secret for the cloud-storage-interface. We have created the token in the preparation step . In my example configuration I have named the token cloud-storage-interface in the hetzner cloud. Create a new deployment file: mkdir -p deployments/csi nano deployments/csi/secret.yml And add the following content: 1 2 3 4 5 6 7 apiVersion : v1 kind : Secret metadata : name : hetzner-container-storage-interface namespace : kube-system stringData : token : \"CLOUD_API_TOKEN_HERE\" #(1)! Replace CLOUD_API_TOKEN_HERE with the token you created in the prerequisite step. The token is named cloud-storage-interface in this example. Apply the secret to the kubernetes cluster by running the following command on your local machine: kubectl apply -f deployments/csi/secret.yml","title":"Setup Secret"},{"location":"deployment/required/csi/#deploy-hcloud-csi","text":"Download the latest version of the storage driver deployment into the deployments/csi folder on your local machine: curl https://raw.githubusercontent.com/hetznercloud/csi-driver/v1.6.0/deploy/kubernetes/hcloud-csi.yml --create-dirs -L -o deployments/csi/deployment.yml Edit the deployment file and replace the secret name. You can use the following command to do this: sed -i 's/^.\\{18\\}name: hcloud-csi$/ name: hetzner-container-storage-interface/' deployments/csi/deployment.yml You can deploy the storage-interface with the following command from your local machine: kubectl apply -f deployments/csi/deployment.yml After this step you should see pods comming up in the cluster. To validate the starting pods, run the following command: kubectl get pods -n kube-system #(1)! You have to use the kube-system namespace here, because the storage-interface is deployed in this namespace.","title":"Deploy hcloud-csi"},{"location":"deployment/required/metrics-server/","text":"Deploy Metrics-Server Kubernetes uses the metrics-server for internal pod-metrics. It is not used for service metrics, these can later be added by other deployments like prometheus, node-exporter and grafana. To deploy the metrics-server download the high-available deployment file to your local machine with the following command: curl https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability.yaml --create-dirs -L -o deployments/metrics-server/deployment.yml Change the deployment file with the following commands to fit our needs: sed -z --in-place 's|replicas: 2|replicas: 3|g' deployments/metrics-server/deployment.yml sed -z --in-place 's| spec:\\n affinity:| spec:\\n tolerations:\\n - key: CriticalAddonsOnly\\n operator: Exists\\n affinity:|g' deployments/metrics-server/deployment.yml sed -z --in-place 's| topologyKey: kubernetes.io/hostname\\n containers:| topologyKey: kubernetes.io/hostname\\n nodeAffinity:\\n requiredDuringSchedulingIgnoredDuringExecution:\\n nodeSelectorTerms:\\n - matchExpressions:\\n - key: node-role.kubernetes.io/master\\n operator: Exists\\n containers:|g' deployments/metrics-server/deployment.yml sed -z --in-place 's|apiVersion: policy/v1beta1|apiVersion: policy/v1|g' deployments/metrics-server/deployment.yml Deploy the metrics-server with the following command: kubectl apply -f deployments/metrics-server/deployment.yml After deploying the metrics-server the pods and nodes can collect internal metrics used in later steps for autoscaling nodes and pods.","title":"Metrics-Server"},{"location":"deployment/required/metrics-server/#deploy-metrics-server","text":"Kubernetes uses the metrics-server for internal pod-metrics. It is not used for service metrics, these can later be added by other deployments like prometheus, node-exporter and grafana. To deploy the metrics-server download the high-available deployment file to your local machine with the following command: curl https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability.yaml --create-dirs -L -o deployments/metrics-server/deployment.yml Change the deployment file with the following commands to fit our needs: sed -z --in-place 's|replicas: 2|replicas: 3|g' deployments/metrics-server/deployment.yml sed -z --in-place 's| spec:\\n affinity:| spec:\\n tolerations:\\n - key: CriticalAddonsOnly\\n operator: Exists\\n affinity:|g' deployments/metrics-server/deployment.yml sed -z --in-place 's| topologyKey: kubernetes.io/hostname\\n containers:| topologyKey: kubernetes.io/hostname\\n nodeAffinity:\\n requiredDuringSchedulingIgnoredDuringExecution:\\n nodeSelectorTerms:\\n - matchExpressions:\\n - key: node-role.kubernetes.io/master\\n operator: Exists\\n containers:|g' deployments/metrics-server/deployment.yml sed -z --in-place 's|apiVersion: policy/v1beta1|apiVersion: policy/v1|g' deployments/metrics-server/deployment.yml Deploy the metrics-server with the following command: kubectl apply -f deployments/metrics-server/deployment.yml After deploying the metrics-server the pods and nodes can collect internal metrics used in later steps for autoscaling nodes and pods.","title":"Deploy Metrics-Server"},{"location":"deployment/required/traefik/","text":"Ingress Controller (traefik) We will use traefik in this example as \"edge router\" and ingress controller. You can find more about traefik on the official traefik website. Prerequisites You need the helm repository from traefik added to your local machine. You can add the repository with the following command: helm repo add traefik https://helm.traefik.io/traefik helm repo update To separate the trafik installation from other deployments we create an own namespace for the trafik pods with the following command: kubectl create namespace traefik Configure Helm Values Create a new helm values file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/values.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 globalArguments : - \"--global.sendanonymoususage=false\" - \"--global.checknewversion=false\" additionalArguments : # - \"--serversTransport.insecureSkipVerify=true\" - \"--log.level=INFO\" - \"--entryPoints.web.proxyProtocol.trustedIPs=10.0.0.200\" - \"--entryPoints.websecure.proxyProtocol.trustedIPs=10.0.0.200\" deployment : enabled : true replicas : 3 annotations : {} podAnnotations : {} additionalContainers : [] initContainers : [] ports : web : redirectTo : websecure websecure : tls : enabled : true ingressRoute : dashboard : enabled : false providers : kubernetesCRD : enabled : true ingressClass : traefik-external kubernetesIngress : enabled : true publishedService : enabled : false rbac : enabled : true service : enabled : true type : LoadBalancer annotations : { load-balancer.hetzner.cloud/name : k8s-ingress #(1)! load-balancer.hetzner.cloud/location : nbg1 load-balancer.hetzner.cloud/use-private-ip : true } labels : {} loadBalancerSourceRanges : [] externalIPs : [] If you changed the name of the ingress loadbalancer you have to change the name here too. Deploy Workload Finally install trafik with the following command run from your local machine: helm install --namespace = traefik traefik traefik/traefik --values = deployments/traefik/values.yml To validate all running services in the cluster, run the following command: kubectl get svc --all-namespaces -o wide Attention You need to check if the loadbalancer is connected with the k8s-service. View the annotations of the service with kubectl describe service traefik -n traefik and check the annotations. You should see the annotations given in the helm-values file. If there are no annotations, you have to add them manually with the following commands: kubectl annotate service traefik load-balancer.hetzner.cloud/use-private-ip = true -n traefik kubectl annotate service traefik load-balancer.hetzner.cloud/name = k8s-ingress -n traefik kubectl annotate service traefik load-balancer.hetzner.cloud/location = nbg1 -n traefik Setup default Middleware Create a new middleware file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/default-middleware.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : default-headers namespace : default spec : headers : browserXssFilter : true contentTypeNosniff : true forceSTSHeader : true stsIncludeSubdomains : true stsPreload : true stsSeconds : 15552000 customFrameOptionsValue : SAMEORIGIN customRequestHeaders : X-Forwarded-Proto : https Change the values that they fot your personal needs. To apply the default middleware, run the following kubectl command: kubectl apply -f deployments/traefik/default-middleware.yml Dashboard To visit all routes traefik provides a dashboard. In the next steps we will create authentication values for the dashboard, a dashboard middleware and the ingressroute to serve traffic to the dashboard. Create Basic-Auth In a previous step we installed apache2-utils to our local machine. With this package you get access to htpassword which we will use now to generate the basic auth credentials. To generate a base64 encoded combination of the username and password, run the following command on your local machine: htpasswd -nb USERNAME PASSWORD | openssl base64 #(1)! Replace USERNAME with your username and PASSWORD with your password. Create a new secret file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-secret.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : traefik-dashboard-auth namespace : traefik type : Opaque data : users : BASE64_ENCODED_USER_AND_PASSWORD_HERE #(1)! Replace BASE64_ENCODED_USER_AND_PASSWORD_HERE with the base64 encoded combination of the username and password you generated in the previous step. As final step apply the dashboard-secret to the kubernetes cluster with the following command: kubectl apply -f deployments/traefik/dashboard-secret.yml Setup Middleware To connect the traefik dashboard with the basic auth created in the previous step we need to create a middleware. Create a new middleware file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-middleware.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : traefik-dashboard-basicauth namespace : traefik spec : basicAuth : secret : traefik-dashboard-auth Apply the middleware to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-middleware.yml Create IngressRoute To serve traffic to the dashboard we need to create an IngressRoute. Create a new ingressroute file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-ingressroute.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : traefik-dashboard namespace : traefik annotations : kubernetes.io/ingress.class : traefik-external spec : entryPoints : - websecure routes : - match : Host(`traefik.example.com`) #(1)! kind : Rule middlewares : - name : traefik-dashboard-basicauth namespace : traefik services : - name : api@internal kind : TraefikService #(2)! # tls: # secretName: example-com-staging-tls Replace traefik.example.com with your domain name you want to use for traefik. The tls setting should be commented out, this will be added when cert-manager is installed and configured. Apply the ingress route to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Remember Remember to add your dns-record to your dns-provider and point it to the loadbalancer ip address. It was referenced in the dns-provider setup early. Connect to your traefik domain (in this example traefik.example.com ) and login with your basic auth credentials you've setup in the previous step . You should see the traefik dashboard with the default middlewares and services.","title":"Ingress Controller"},{"location":"deployment/required/traefik/#ingress-controller-traefik","text":"We will use traefik in this example as \"edge router\" and ingress controller. You can find more about traefik on the official traefik website.","title":"Ingress Controller (traefik)"},{"location":"deployment/required/traefik/#prerequisites","text":"You need the helm repository from traefik added to your local machine. You can add the repository with the following command: helm repo add traefik https://helm.traefik.io/traefik helm repo update To separate the trafik installation from other deployments we create an own namespace for the trafik pods with the following command: kubectl create namespace traefik","title":"Prerequisites"},{"location":"deployment/required/traefik/#configure-helm-values","text":"Create a new helm values file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/values.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 globalArguments : - \"--global.sendanonymoususage=false\" - \"--global.checknewversion=false\" additionalArguments : # - \"--serversTransport.insecureSkipVerify=true\" - \"--log.level=INFO\" - \"--entryPoints.web.proxyProtocol.trustedIPs=10.0.0.200\" - \"--entryPoints.websecure.proxyProtocol.trustedIPs=10.0.0.200\" deployment : enabled : true replicas : 3 annotations : {} podAnnotations : {} additionalContainers : [] initContainers : [] ports : web : redirectTo : websecure websecure : tls : enabled : true ingressRoute : dashboard : enabled : false providers : kubernetesCRD : enabled : true ingressClass : traefik-external kubernetesIngress : enabled : true publishedService : enabled : false rbac : enabled : true service : enabled : true type : LoadBalancer annotations : { load-balancer.hetzner.cloud/name : k8s-ingress #(1)! load-balancer.hetzner.cloud/location : nbg1 load-balancer.hetzner.cloud/use-private-ip : true } labels : {} loadBalancerSourceRanges : [] externalIPs : [] If you changed the name of the ingress loadbalancer you have to change the name here too.","title":"Configure Helm Values"},{"location":"deployment/required/traefik/#deploy-workload","text":"Finally install trafik with the following command run from your local machine: helm install --namespace = traefik traefik traefik/traefik --values = deployments/traefik/values.yml To validate all running services in the cluster, run the following command: kubectl get svc --all-namespaces -o wide Attention You need to check if the loadbalancer is connected with the k8s-service. View the annotations of the service with kubectl describe service traefik -n traefik and check the annotations. You should see the annotations given in the helm-values file. If there are no annotations, you have to add them manually with the following commands: kubectl annotate service traefik load-balancer.hetzner.cloud/use-private-ip = true -n traefik kubectl annotate service traefik load-balancer.hetzner.cloud/name = k8s-ingress -n traefik kubectl annotate service traefik load-balancer.hetzner.cloud/location = nbg1 -n traefik","title":"Deploy Workload"},{"location":"deployment/required/traefik/#setup-default-middleware","text":"Create a new middleware file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/default-middleware.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : default-headers namespace : default spec : headers : browserXssFilter : true contentTypeNosniff : true forceSTSHeader : true stsIncludeSubdomains : true stsPreload : true stsSeconds : 15552000 customFrameOptionsValue : SAMEORIGIN customRequestHeaders : X-Forwarded-Proto : https Change the values that they fot your personal needs. To apply the default middleware, run the following kubectl command: kubectl apply -f deployments/traefik/default-middleware.yml","title":"Setup default Middleware"},{"location":"deployment/required/traefik/#dashboard","text":"To visit all routes traefik provides a dashboard. In the next steps we will create authentication values for the dashboard, a dashboard middleware and the ingressroute to serve traffic to the dashboard.","title":"Dashboard"},{"location":"deployment/required/traefik/#create-basic-auth","text":"In a previous step we installed apache2-utils to our local machine. With this package you get access to htpassword which we will use now to generate the basic auth credentials. To generate a base64 encoded combination of the username and password, run the following command on your local machine: htpasswd -nb USERNAME PASSWORD | openssl base64 #(1)! Replace USERNAME with your username and PASSWORD with your password. Create a new secret file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-secret.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : v1 kind : Secret metadata : name : traefik-dashboard-auth namespace : traefik type : Opaque data : users : BASE64_ENCODED_USER_AND_PASSWORD_HERE #(1)! Replace BASE64_ENCODED_USER_AND_PASSWORD_HERE with the base64 encoded combination of the username and password you generated in the previous step. As final step apply the dashboard-secret to the kubernetes cluster with the following command: kubectl apply -f deployments/traefik/dashboard-secret.yml","title":"Create Basic-Auth"},{"location":"deployment/required/traefik/#setup-middleware","text":"To connect the traefik dashboard with the basic auth created in the previous step we need to create a middleware. Create a new middleware file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-middleware.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : traefik-dashboard-basicauth namespace : traefik spec : basicAuth : secret : traefik-dashboard-auth Apply the middleware to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-middleware.yml","title":"Setup Middleware"},{"location":"deployment/required/traefik/#create-ingressroute","text":"To serve traffic to the dashboard we need to create an IngressRoute. Create a new ingressroute file for traefik with the following command: mkdir -p deployments/traefik nano deployments/traefik/dashboard-ingressroute.yml Edit the file and add the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : traefik-dashboard namespace : traefik annotations : kubernetes.io/ingress.class : traefik-external spec : entryPoints : - websecure routes : - match : Host(`traefik.example.com`) #(1)! kind : Rule middlewares : - name : traefik-dashboard-basicauth namespace : traefik services : - name : api@internal kind : TraefikService #(2)! # tls: # secretName: example-com-staging-tls Replace traefik.example.com with your domain name you want to use for traefik. The tls setting should be commented out, this will be added when cert-manager is installed and configured. Apply the ingress route to the cluster with the following command: kubectl apply -f deployments/traefik/dashboard-ingressroute.yml Remember Remember to add your dns-record to your dns-provider and point it to the loadbalancer ip address. It was referenced in the dns-provider setup early. Connect to your traefik domain (in this example traefik.example.com ) and login with your basic auth credentials you've setup in the previous step . You should see the traefik dashboard with the default middlewares and services.","title":"Create IngressRoute"},{"location":"deployment/required/upgrade-controller/","text":"Upgrade-Controller To upgrade the kubernetes cluster, we need to deploy the upgrade-controller. This controller will check for new kubernetes versions and upgrade the cluster if a new version is available. You can deploy different update strategies to the cluster to keep a working cluster during the upgrade. You can download the latest version of the upgrade-controller deployment into the deployments/upgrade-controller folder on your local machine: curl https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml --create-dirs -L -o deployments/upgrade-controller/deployment.yml You can deploy the upgrade-controller with the following command from your local machine: kubectl apply -f deployments/upgrade-controller/deployment.yml For the upgrade-controller to work, we need to create a configmap with the upgrade-strategy. Visit the official documentation for example upgrade plans .","title":"Upgrade Controller"},{"location":"deployment/required/upgrade-controller/#upgrade-controller","text":"To upgrade the kubernetes cluster, we need to deploy the upgrade-controller. This controller will check for new kubernetes versions and upgrade the cluster if a new version is available. You can deploy different update strategies to the cluster to keep a working cluster during the upgrade. You can download the latest version of the upgrade-controller deployment into the deployments/upgrade-controller folder on your local machine: curl https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml --create-dirs -L -o deployments/upgrade-controller/deployment.yml You can deploy the upgrade-controller with the following command from your local machine: kubectl apply -f deployments/upgrade-controller/deployment.yml For the upgrade-controller to work, we need to create a configmap with the upgrade-strategy. Visit the official documentation for example upgrade plans .","title":"Upgrade-Controller"},{"location":"home/","text":"Home Introduction In this documentation you will find a step by step solution to deploy a high-available, auto scalable and loadbalanced k3s cluster to servers inside the Hetzner-Cloud. This guide covers: Prerequisites Setting up Hetzner account (including cloud project, api-tokens and ssh-keys) Setting up Docker-Hub as private container repository for custom images (e.g. cluster-autoscaler) Setting up CloudFlare as dns provider for dns01 acme challenge Installing all requirements on your local development machine ( hcloud command line , helm , kubectl , go and docker ) Installation Setting up placement groups to separate hetzner cloud servers Setting up private networks for communication between servers Setting up three k3s-servers (controlplane) in high-available mode Create a hetzner loadbalancer to loadbalance the kubernetes-api Create a hetzner loadbalancer to loadbalance the services exposed to the ingress Install all required packages to the hosts and configure them properly Automatic install and configuration of k3s with one single command Deployment Deploy the hetzner cloud-controller-manager to the cluster to integrate the hetzner api into the k8s cluster Deploy the hetzner container-storage-interface to the cluster to use hetzner cloud volumes as persistent volume claim inside k8s Deploy the cluster-autoscaler to the cluster and configure it to use hetzner cloud servers for automatic scaleup and scaledown of nodes Deploy the metrics-server to get metrics from pods and nodes and enable autoscaling of nodes and pods Deploy the system-upgrade-controller to use automated upgrade plans Deploy and configure traefik as ingress controller Deploy and configure cert-manager to use letsencrypt certificates for all services exposed to the internet Setting up the traefik dashboard with basic auth and certificates Give an example of horizontal pod autoscaling to scale pods based on cpu usage Files By following this guide you will need deployment files to change them to fit your needs and to deploy them to your cluster. You will find the the original files references in the download command. Disclaimer This guide is not an official guide from Hetzner or k3s. If you want help, do not contant the official support of Hetzner or k3s. Instead you can open an issue in the github repository .","title":"Home"},{"location":"home/#home","text":"","title":"Home"},{"location":"home/#introduction","text":"In this documentation you will find a step by step solution to deploy a high-available, auto scalable and loadbalanced k3s cluster to servers inside the Hetzner-Cloud. This guide covers: Prerequisites Setting up Hetzner account (including cloud project, api-tokens and ssh-keys) Setting up Docker-Hub as private container repository for custom images (e.g. cluster-autoscaler) Setting up CloudFlare as dns provider for dns01 acme challenge Installing all requirements on your local development machine ( hcloud command line , helm , kubectl , go and docker ) Installation Setting up placement groups to separate hetzner cloud servers Setting up private networks for communication between servers Setting up three k3s-servers (controlplane) in high-available mode Create a hetzner loadbalancer to loadbalance the kubernetes-api Create a hetzner loadbalancer to loadbalance the services exposed to the ingress Install all required packages to the hosts and configure them properly Automatic install and configuration of k3s with one single command Deployment Deploy the hetzner cloud-controller-manager to the cluster to integrate the hetzner api into the k8s cluster Deploy the hetzner container-storage-interface to the cluster to use hetzner cloud volumes as persistent volume claim inside k8s Deploy the cluster-autoscaler to the cluster and configure it to use hetzner cloud servers for automatic scaleup and scaledown of nodes Deploy the metrics-server to get metrics from pods and nodes and enable autoscaling of nodes and pods Deploy the system-upgrade-controller to use automated upgrade plans Deploy and configure traefik as ingress controller Deploy and configure cert-manager to use letsencrypt certificates for all services exposed to the internet Setting up the traefik dashboard with basic auth and certificates Give an example of horizontal pod autoscaling to scale pods based on cpu usage","title":"Introduction"},{"location":"home/#files","text":"By following this guide you will need deployment files to change them to fit your needs and to deploy them to your cluster. You will find the the original files references in the download command.","title":"Files"},{"location":"home/#disclaimer","text":"This guide is not an official guide from Hetzner or k3s. If you want help, do not contant the official support of Hetzner or k3s. Instead you can open an issue in the github repository .","title":"Disclaimer"},{"location":"home/credits/","text":"Credits Huge thank you to many people and git repos where I got the information and commands from. Special thanks to: Techno Tim The DevOps Guy Hetzner Development Team Also special thanks to all contributors of the repository.","title":"Credits"},{"location":"home/credits/#credits","text":"Huge thank you to many people and git repos where I got the information and commands from. Special thanks to: Techno Tim The DevOps Guy Hetzner Development Team Also special thanks to all contributors of the repository.","title":"Credits"},{"location":"home/todo/","text":"ToDo You can see all the things that need to be done in the GitHub issues .","title":"ToDo"},{"location":"home/todo/#todo","text":"You can see all the things that need to be done in the GitHub issues .","title":"ToDo"},{"location":"home/troubleshooting/","text":"Troubleshooting If errors occur during the installation, please be shure that you have followed the instructions carefully. If you are still having problems, please open an issue on the GitHub repository .","title":"Troubleshooting"},{"location":"home/troubleshooting/#troubleshooting","text":"If errors occur during the installation, please be shure that you have followed the instructions carefully. If you are still having problems, please open an issue on the GitHub repository .","title":"Troubleshooting"},{"location":"installation/","text":"Installation In this step we will setup all servers, install needed packages and finally install k3s.","title":"Installation"},{"location":"installation/#installation","text":"In this step we will setup all servers, install needed packages and finally install k3s.","title":"Installation"},{"location":"installation/hetzner/","text":"Hetzner To provide servers, a network-connection and load-balancers we will use the hetzner cloud. In this step we will create all parts for the hetzner infrastructure. Create Placement-Groups To separate all servers from each other, we will create placement groups for the servers. One placement group will be for one server-role for one location. This will result in 6 placement groups (3 locations (hel-1, fsn-1, nbg-1) * 2 server-roles (server and agent)) To create all placement groups, run the following commands on your local machine: # Create placement group for servers in hel-1 hcloud placement-group create --type spread --name k8s-control_plane-hel1 --label k8s-role = control_plane --label location = hel1 # Create placement group for servers in fsn-1 hcloud placement-group create --type spread --name k8s-control_plane-fsn1 --label k8s-role = control_plane --label location = fsn1 # Create placement group for servers in nbg-1 hcloud placement-group create --type spread --name k8s-control_plane-nbg1 --label k8s-role = control_plane --label location = nbg1 # Create placement group for agents in hel-1 hcloud placement-group create --type spread --name k8s-agent-hel1 --label k8s-role = agent --label location = hel1 # Create placement group for agents in fsn-1 hcloud placement-group create --type spread --name k8s-agent-fsn1 --label k8s-role = agent --label location = fsn1 # Create placement group for agents in nbg-1 hcloud placement-group create --type spread --name k8s-agent-nbg1 --label k8s-role = agent --label location = nbg1 Attention The placement groups used for the agents are not used yet by the cluster-autoscaler. The commands will create the following placement groups: 3 placement groups for the control-plane servers with the name k8s-control_plane-{location} and the labels k8s-role=control_plane and location={location} 3 placement groups for the agents with the name k8s-agent-{location} and the labels k8s-role=agent and location={location} The {location} will be replaced with the location of the placement group. The locations are hel1 , fsn1 and nbg1 . Create private Network We will use a hetzner cloud network to connect all servers to each other ane enable the load-balancers to connect to the private ips of the servers. To create the private network for the servers run the following command on your local machine: hcloud network create --name k8s --ip-range 10 .0.0.0/8 --label k8s-role = control_plane-agent --label location = hel1-fsn1-nbg1 The command will create a network with the name k8s and the labels k8s-role=control_plane-agent and location=hel1-fsn1-nbg1 . To create the separate subnets inside this network, run the following commands on your local machine: # Create subnet for loadbalancers (for kubernetes-api (controlplane) and ingress (services)) hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .0.0.0/24 # Create subnet for servers (control-plane) in hel-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.0.0/24 # Create subnet for servers (control-plane) in fsn-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.1.0/24 # Create subnet for servers (control-plane) in nbg-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.2.0/24 # Create subnet for agents in hel-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.0.0/24 # Create subnet for agents in fsn-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.1.0/24 # Create subnet for agent in nbg-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.2.0/24 The commands will create the following subnets: 10.0.0.0/24 for the load balancers for the controlplane and agents 10.1.0.0/24 for the controlplane in hel1 10.1.1.0/24 for the controlplane in fsn1 10.1.2.0/24 for the controlplane in nbg1 10.2.0.0/24 for the agents in hel1 10.2.1.0/24 for the agents in fsn1 10.2.2.0/24 for the agents in nbg1 Create Servers To create the servers for the control-plane, run the following commands on your local machine: Replace values You have to replace the --ssh-key SSH_KEY_NAME with your ssh-key name you have uploaded in the prerequisite step. If you want to use more than one ssh-key , you can specify the ssh-key parameter multiple times . # Create server for control-plane in hel-1 hcloud server create \\ --datacenter hel1-dc2 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-hel1 \\ --name k8s-controlplane-hel1-1 \\ --label k8s-role = control_plane \\ --label location = hel1 # Create server for control-plane in fsn-1 hcloud server create \\ --datacenter fsn1-dc14 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-fsn1 \\ --name k8s-controlplane-fsn1-1 \\ --label k8s-role = control_plane \\ --label location = fsn1 # Create server for control-plane in nbg-1 hcloud server create \\ --datacenter nbg1-dc3 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-nbg1 \\ --name k8s-controlplane-nbg1-1 \\ --label k8s-role = control_plane \\ --label location = nbg1 The commands will create a control plane node in each hetzner location with... the name k8s-controlplane-{location}-1 the server type CX21 (2 cores, 4gb ram) the image debian-11 the ssh-key added in the prerequisite step the placement-groups created at the top of this page and the labels k8s-role=control_plane and location={location} . To add the servers to the private network, run the following commands on your local machine: hcloud server attach-to-network k8s-controlplane-hel1-1 --network k8s --ip 10 .1.0.1 hcloud server attach-to-network k8s-controlplane-fsn1-1 --network k8s --ip 10 .1.1.1 hcloud server attach-to-network k8s-controlplane-nbg1-1 --network k8s --ip 10 .1.2.1 The commands will add the servers to the private network k8s (created in a previous step) and assign the following ips: 10.1.0.1 to the control plane in hel1 10.1.1.1 to the control plane in fsn1 10.1.2.1 to the control plane in nbg1 See also network creation in a previous step . Create Loadbalancers Kubernetes needs two loadbalancers. One for the control plane and one for the ingress. In this setup we will use external hardware loadbalancers from the hetzner cloud. Loadbalancer for control plane So in this step we will create the loadbalancer for the control plane with executing the following commands on your local machine: # Create loadbalancer for control plane hcloud load-balancer create --algorithm-type round_robin --location fsn1 --name k8s-controlplane --type lb11 --label k8s-role = control_plane --label location = fsn1 # Attach the loadbalancer to the private network hcloud load-balancer attach-to-network k8s-controlplane --network k8s --ip 10 .0.0.100 # Add target servers to the loadbalancer hcloud load-balancer add-target k8s-controlplane --label-selector k8s-role = control_plane --use-private-ip # Create service for the loadbalancer hcloud load-balancer add-service k8s-controlplane --destination-port 6443 --listen-port 6443 --protocol tcp The commands will create the loadbalancer for the controlplane with the following configuration: Name: k8s-controlplane Location: fsn1 Type: lb11 (max services: 5, max connections: 10000, max targets: 25) Algorithm: round_robin private ip 10.0.0.100 inside the previously created Network labels k8s-role = control_plane and location = fsn1 add all servers with label k8s-role = control_plane as target expose port 6443 for the control plane Loadbalancer for Ingress In this step we will create the loadbalancer for the ingress with executing the following commands on your local machine: # Create loadbalancer for ingress hcloud load-balancer create --algorithm-type round_robin --location nbg1 --name k8s-ingress --type lb11 --label k8s-role = agent --label location = nbg1 # Attach the loadbalancer to the private network hcloud load-balancer attach-to-network k8s-ingress --network k8s --ip 10 .0.0.200 # Add target servers to the loadbalancer hcloud load-balancer add-target k8s-ingress --label-selector k8s-role = agent --use-private-ip The commands will create the loadbalancer for the ingress with the following configuration: Name: k8s-ingress Location: nbg1 Type: lb11 (max services: 5, max connections: 10000, max targets: 25) Algorithm: round_robin private ip 10.0.0.200 inside the previously created Network labels k8s-role = agent and location = nbg1 add all servers with label k8s-role = agent as target Loadbalancer will not expose any ports because the ingress controller will do that later automatically","title":"Hetzner"},{"location":"installation/hetzner/#hetzner","text":"To provide servers, a network-connection and load-balancers we will use the hetzner cloud. In this step we will create all parts for the hetzner infrastructure.","title":"Hetzner"},{"location":"installation/hetzner/#create-placement-groups","text":"To separate all servers from each other, we will create placement groups for the servers. One placement group will be for one server-role for one location. This will result in 6 placement groups (3 locations (hel-1, fsn-1, nbg-1) * 2 server-roles (server and agent)) To create all placement groups, run the following commands on your local machine: # Create placement group for servers in hel-1 hcloud placement-group create --type spread --name k8s-control_plane-hel1 --label k8s-role = control_plane --label location = hel1 # Create placement group for servers in fsn-1 hcloud placement-group create --type spread --name k8s-control_plane-fsn1 --label k8s-role = control_plane --label location = fsn1 # Create placement group for servers in nbg-1 hcloud placement-group create --type spread --name k8s-control_plane-nbg1 --label k8s-role = control_plane --label location = nbg1 # Create placement group for agents in hel-1 hcloud placement-group create --type spread --name k8s-agent-hel1 --label k8s-role = agent --label location = hel1 # Create placement group for agents in fsn-1 hcloud placement-group create --type spread --name k8s-agent-fsn1 --label k8s-role = agent --label location = fsn1 # Create placement group for agents in nbg-1 hcloud placement-group create --type spread --name k8s-agent-nbg1 --label k8s-role = agent --label location = nbg1 Attention The placement groups used for the agents are not used yet by the cluster-autoscaler. The commands will create the following placement groups: 3 placement groups for the control-plane servers with the name k8s-control_plane-{location} and the labels k8s-role=control_plane and location={location} 3 placement groups for the agents with the name k8s-agent-{location} and the labels k8s-role=agent and location={location} The {location} will be replaced with the location of the placement group. The locations are hel1 , fsn1 and nbg1 .","title":"Create Placement-Groups"},{"location":"installation/hetzner/#create-private-network","text":"We will use a hetzner cloud network to connect all servers to each other ane enable the load-balancers to connect to the private ips of the servers. To create the private network for the servers run the following command on your local machine: hcloud network create --name k8s --ip-range 10 .0.0.0/8 --label k8s-role = control_plane-agent --label location = hel1-fsn1-nbg1 The command will create a network with the name k8s and the labels k8s-role=control_plane-agent and location=hel1-fsn1-nbg1 . To create the separate subnets inside this network, run the following commands on your local machine: # Create subnet for loadbalancers (for kubernetes-api (controlplane) and ingress (services)) hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .0.0.0/24 # Create subnet for servers (control-plane) in hel-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.0.0/24 # Create subnet for servers (control-plane) in fsn-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.1.0/24 # Create subnet for servers (control-plane) in nbg-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .1.2.0/24 # Create subnet for agents in hel-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.0.0/24 # Create subnet for agents in fsn-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.1.0/24 # Create subnet for agent in nbg-1 hcloud network add-subnet k8s --network-zone eu-central --type cloud --ip-range 10 .2.2.0/24 The commands will create the following subnets: 10.0.0.0/24 for the load balancers for the controlplane and agents 10.1.0.0/24 for the controlplane in hel1 10.1.1.0/24 for the controlplane in fsn1 10.1.2.0/24 for the controlplane in nbg1 10.2.0.0/24 for the agents in hel1 10.2.1.0/24 for the agents in fsn1 10.2.2.0/24 for the agents in nbg1","title":"Create private Network"},{"location":"installation/hetzner/#create-servers","text":"To create the servers for the control-plane, run the following commands on your local machine: Replace values You have to replace the --ssh-key SSH_KEY_NAME with your ssh-key name you have uploaded in the prerequisite step. If you want to use more than one ssh-key , you can specify the ssh-key parameter multiple times . # Create server for control-plane in hel-1 hcloud server create \\ --datacenter hel1-dc2 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-hel1 \\ --name k8s-controlplane-hel1-1 \\ --label k8s-role = control_plane \\ --label location = hel1 # Create server for control-plane in fsn-1 hcloud server create \\ --datacenter fsn1-dc14 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-fsn1 \\ --name k8s-controlplane-fsn1-1 \\ --label k8s-role = control_plane \\ --label location = fsn1 # Create server for control-plane in nbg-1 hcloud server create \\ --datacenter nbg1-dc3 \\ --image debian-11 \\ --ssh-key SSH_KEY_NAME \\ --type cx21 \\ --placement-group k8s-control_plane-nbg1 \\ --name k8s-controlplane-nbg1-1 \\ --label k8s-role = control_plane \\ --label location = nbg1 The commands will create a control plane node in each hetzner location with... the name k8s-controlplane-{location}-1 the server type CX21 (2 cores, 4gb ram) the image debian-11 the ssh-key added in the prerequisite step the placement-groups created at the top of this page and the labels k8s-role=control_plane and location={location} . To add the servers to the private network, run the following commands on your local machine: hcloud server attach-to-network k8s-controlplane-hel1-1 --network k8s --ip 10 .1.0.1 hcloud server attach-to-network k8s-controlplane-fsn1-1 --network k8s --ip 10 .1.1.1 hcloud server attach-to-network k8s-controlplane-nbg1-1 --network k8s --ip 10 .1.2.1 The commands will add the servers to the private network k8s (created in a previous step) and assign the following ips: 10.1.0.1 to the control plane in hel1 10.1.1.1 to the control plane in fsn1 10.1.2.1 to the control plane in nbg1 See also network creation in a previous step .","title":"Create Servers"},{"location":"installation/hetzner/#create-loadbalancers","text":"Kubernetes needs two loadbalancers. One for the control plane and one for the ingress. In this setup we will use external hardware loadbalancers from the hetzner cloud.","title":"Create Loadbalancers"},{"location":"installation/hetzner/#loadbalancer-for-control-plane","text":"So in this step we will create the loadbalancer for the control plane with executing the following commands on your local machine: # Create loadbalancer for control plane hcloud load-balancer create --algorithm-type round_robin --location fsn1 --name k8s-controlplane --type lb11 --label k8s-role = control_plane --label location = fsn1 # Attach the loadbalancer to the private network hcloud load-balancer attach-to-network k8s-controlplane --network k8s --ip 10 .0.0.100 # Add target servers to the loadbalancer hcloud load-balancer add-target k8s-controlplane --label-selector k8s-role = control_plane --use-private-ip # Create service for the loadbalancer hcloud load-balancer add-service k8s-controlplane --destination-port 6443 --listen-port 6443 --protocol tcp The commands will create the loadbalancer for the controlplane with the following configuration: Name: k8s-controlplane Location: fsn1 Type: lb11 (max services: 5, max connections: 10000, max targets: 25) Algorithm: round_robin private ip 10.0.0.100 inside the previously created Network labels k8s-role = control_plane and location = fsn1 add all servers with label k8s-role = control_plane as target expose port 6443 for the control plane","title":"Loadbalancer for control plane"},{"location":"installation/hetzner/#loadbalancer-for-ingress","text":"In this step we will create the loadbalancer for the ingress with executing the following commands on your local machine: # Create loadbalancer for ingress hcloud load-balancer create --algorithm-type round_robin --location nbg1 --name k8s-ingress --type lb11 --label k8s-role = agent --label location = nbg1 # Attach the loadbalancer to the private network hcloud load-balancer attach-to-network k8s-ingress --network k8s --ip 10 .0.0.200 # Add target servers to the loadbalancer hcloud load-balancer add-target k8s-ingress --label-selector k8s-role = agent --use-private-ip The commands will create the loadbalancer for the ingress with the following configuration: Name: k8s-ingress Location: nbg1 Type: lb11 (max services: 5, max connections: 10000, max targets: 25) Algorithm: round_robin private ip 10.0.0.200 inside the previously created Network labels k8s-role = agent and location = nbg1 add all servers with label k8s-role = agent as target Loadbalancer will not expose any ports because the ingress controller will do that later automatically","title":"Loadbalancer for Ingress"},{"location":"installation/k3s/","text":"k3s In this step we will finally install k3s on the servers. Token First we need to generate a token for the server to join the cluster. This token is used to authenticate the server to the cluster. We can generate a token with the following command: openssl rand -hex 64 Be shure to save the token in a save place , you will need it later to join the servers and agents to the cluster. control-plane To install the k3s control-plane software on the control-plane host, we have to separate the installation to the first installed control-plane and the other control-planes. Install first Server To install k3s on the first control-plane node (in this example control-plane-fsn1-1), run the following command on the server: Replace values Please replace the K3S_TOKEN_HERE with your previously created k3s-token and the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane created in the hetzner step . curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION = \"v1.25.0-rc1+k3s1\" \\ K3S_TOKEN = \"K3S_TOKEN_HERE\" \\ INSTALL_K3S_EXEC = \"server \\ --disable-cloud-controller \\ --disable servicelb \\ --disable traefik \\ --disable local-storage \\ --disable metrics-server \\ --node-name=\" $( hostname -f ) \" \\ --cluster-cidr=10.100.0.0/16 \\ --etcd-expose-metrics=true \\ --kube-controller-manager-arg=\" bind-address = 0 .0.0.0 \" \\ --kube-proxy-arg=\" metrics-bind-address = 0 .0.0.0 \" \\ --kube-scheduler-arg=\" bind-address = 0 .0.0.0 \" \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --kubelet-arg=\" cloud-provider = external \" \\ --advertise-address= $( hostname -I | awk '{print $2}' ) \\ --node-ip= $( hostname -I | awk '{print $2}' ) \\ --node-external-ip= $( hostname -I | awk '{print $1}' ) \\ --flannel-iface=ens10 \\ --tls-san=LOADBALANCER_PUBLIC_IP_HERE \\ --tls-san=10.0.0.100 \\ --tls-san=10.1.0.1 \\ --tls-san=10.1.1.1 \\ --tls-san=10.1.2.1 \\ --cluster-init\" sh - This installation disables or customises many parameters to fit the needs of this setup. Attention You have to wait for the loadbalancer to report the status \"mixed\". So that the loadbalancer is ready to accept traffic. Otherwise the installation of the other nodes will fail because they cant communicate with the cluster. Install other Servers To install k3s on the other controlplane nodes (in this example control-plane-hel1-1 and control-plane-ngb1-1), run the following command on the server: Replace values Please replace the K3S_TOKEN_HERE with your previously created k3s-token and the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane created in the hetzner step . curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION = \"v1.25.0-rc1+k3s1\" \\ K3S_TOKEN = \"K3S_TOKEN_HERE\" \\ INSTALL_K3S_EXEC = \"server \\ --disable-cloud-controller \\ --disable servicelb \\ --disable traefik \\ --disable local-storage \\ --disable metrics-server \\ --node-name=\" $( hostname -f ) \" \\ --cluster-cidr=10.100.0.0/16 \\ --etcd-expose-metrics=true \\ --kube-controller-manager-arg=\" bind-address = 0 .0.0.0 \" \\ --kube-proxy-arg=\" metrics-bind-address = 0 .0.0.0 \" \\ --kube-scheduler-arg=\" bind-address = 0 .0.0.0 \" \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --kubelet-arg=\" cloud-provider = external \" \\ --advertise-address= $( hostname -I | awk '{print $2}' ) \\ --node-ip= $( hostname -I | awk '{print $2}' ) \\ --node-external-ip= $( hostname -I | awk '{print $1}' ) \\ --flannel-iface=ens10 \\ --tls-san=LOADBALANCER_PUBLIC_IP_HERE \\ --tls-san=10.0.0.100 \\ --tls-san=10.1.0.1 \\ --tls-san=10.1.1.1 \\ --tls-san=10.1.2.1 \\ --server https://10.0.0.100:6443\" sh - This installation also disables and customises many parameters to fit the needs of this setup. The controlplanes will communicate using the private-ip of the controlplane loadbalancer. Setup kubectl To communicate with the kubernetes cluster we use kubectl on our local machine, which we setup in the local machine step . For the authentication between your local machine and the k3s cluster, kubectl uses tokens. These tokens are stored in the kubeconfig file. Run the following command on your local machine to copy the kubeconfig file from one of the controlplane hosts to your local machine: Replace values Please replace the CONTROLPLANE_PUBLIC_IP_HERE with the public ip of one of the controlplane hosts (for example k8s-controlplane-hel1-1). scp root@CONTROLPLANE_PUBLIC_IP_HERE:/etc/rancher/k3s/k3s.yaml ~/.kube/config To replace the localhost ip used in the kubectl file with the public ip of the loadbalancer run the following command. Replace values Please replace the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane. sed -i 's/127.0.0.1/LOADBALANCER_PUBLIC_IP_HERE/' ~/.kube/config As last step change the access rights to the kubeconfig file. Otherwise kubectl will not use the config file because the access rights are too open. chmod 600 ~/.kube/config To check if the communication between the hosts and the local machine works, run the following command on your local machine: kubectl get nodes You should see 3 controlplane nodes in the output.","title":"k3s"},{"location":"installation/k3s/#k3s","text":"In this step we will finally install k3s on the servers.","title":"k3s"},{"location":"installation/k3s/#token","text":"First we need to generate a token for the server to join the cluster. This token is used to authenticate the server to the cluster. We can generate a token with the following command: openssl rand -hex 64 Be shure to save the token in a save place , you will need it later to join the servers and agents to the cluster.","title":"Token"},{"location":"installation/k3s/#control-plane","text":"To install the k3s control-plane software on the control-plane host, we have to separate the installation to the first installed control-plane and the other control-planes.","title":"control-plane"},{"location":"installation/k3s/#install-first-server","text":"To install k3s on the first control-plane node (in this example control-plane-fsn1-1), run the following command on the server: Replace values Please replace the K3S_TOKEN_HERE with your previously created k3s-token and the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane created in the hetzner step . curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION = \"v1.25.0-rc1+k3s1\" \\ K3S_TOKEN = \"K3S_TOKEN_HERE\" \\ INSTALL_K3S_EXEC = \"server \\ --disable-cloud-controller \\ --disable servicelb \\ --disable traefik \\ --disable local-storage \\ --disable metrics-server \\ --node-name=\" $( hostname -f ) \" \\ --cluster-cidr=10.100.0.0/16 \\ --etcd-expose-metrics=true \\ --kube-controller-manager-arg=\" bind-address = 0 .0.0.0 \" \\ --kube-proxy-arg=\" metrics-bind-address = 0 .0.0.0 \" \\ --kube-scheduler-arg=\" bind-address = 0 .0.0.0 \" \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --kubelet-arg=\" cloud-provider = external \" \\ --advertise-address= $( hostname -I | awk '{print $2}' ) \\ --node-ip= $( hostname -I | awk '{print $2}' ) \\ --node-external-ip= $( hostname -I | awk '{print $1}' ) \\ --flannel-iface=ens10 \\ --tls-san=LOADBALANCER_PUBLIC_IP_HERE \\ --tls-san=10.0.0.100 \\ --tls-san=10.1.0.1 \\ --tls-san=10.1.1.1 \\ --tls-san=10.1.2.1 \\ --cluster-init\" sh - This installation disables or customises many parameters to fit the needs of this setup. Attention You have to wait for the loadbalancer to report the status \"mixed\". So that the loadbalancer is ready to accept traffic. Otherwise the installation of the other nodes will fail because they cant communicate with the cluster.","title":"Install first Server"},{"location":"installation/k3s/#install-other-servers","text":"To install k3s on the other controlplane nodes (in this example control-plane-hel1-1 and control-plane-ngb1-1), run the following command on the server: Replace values Please replace the K3S_TOKEN_HERE with your previously created k3s-token and the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane created in the hetzner step . curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION = \"v1.25.0-rc1+k3s1\" \\ K3S_TOKEN = \"K3S_TOKEN_HERE\" \\ INSTALL_K3S_EXEC = \"server \\ --disable-cloud-controller \\ --disable servicelb \\ --disable traefik \\ --disable local-storage \\ --disable metrics-server \\ --node-name=\" $( hostname -f ) \" \\ --cluster-cidr=10.100.0.0/16 \\ --etcd-expose-metrics=true \\ --kube-controller-manager-arg=\" bind-address = 0 .0.0.0 \" \\ --kube-proxy-arg=\" metrics-bind-address = 0 .0.0.0 \" \\ --kube-scheduler-arg=\" bind-address = 0 .0.0.0 \" \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --kubelet-arg=\" cloud-provider = external \" \\ --advertise-address= $( hostname -I | awk '{print $2}' ) \\ --node-ip= $( hostname -I | awk '{print $2}' ) \\ --node-external-ip= $( hostname -I | awk '{print $1}' ) \\ --flannel-iface=ens10 \\ --tls-san=LOADBALANCER_PUBLIC_IP_HERE \\ --tls-san=10.0.0.100 \\ --tls-san=10.1.0.1 \\ --tls-san=10.1.1.1 \\ --tls-san=10.1.2.1 \\ --server https://10.0.0.100:6443\" sh - This installation also disables and customises many parameters to fit the needs of this setup. The controlplanes will communicate using the private-ip of the controlplane loadbalancer.","title":"Install other Servers"},{"location":"installation/k3s/#setup-kubectl","text":"To communicate with the kubernetes cluster we use kubectl on our local machine, which we setup in the local machine step . For the authentication between your local machine and the k3s cluster, kubectl uses tokens. These tokens are stored in the kubeconfig file. Run the following command on your local machine to copy the kubeconfig file from one of the controlplane hosts to your local machine: Replace values Please replace the CONTROLPLANE_PUBLIC_IP_HERE with the public ip of one of the controlplane hosts (for example k8s-controlplane-hel1-1). scp root@CONTROLPLANE_PUBLIC_IP_HERE:/etc/rancher/k3s/k3s.yaml ~/.kube/config To replace the localhost ip used in the kubectl file with the public ip of the loadbalancer run the following command. Replace values Please replace the LOADBALANCER_PUBLIC_IP_HERE with the public ip of the loadbalancer for the controlplane. sed -i 's/127.0.0.1/LOADBALANCER_PUBLIC_IP_HERE/' ~/.kube/config As last step change the access rights to the kubeconfig file. Otherwise kubectl will not use the config file because the access rights are too open. chmod 600 ~/.kube/config To check if the communication between the hosts and the local machine works, run the following command on your local machine: kubectl get nodes You should see 3 controlplane nodes in the output.","title":"Setup kubectl"},{"location":"installation/servers/","text":"Servers (VMs) After creating the servers for the controlplane, we have to configure the operating system. The servers get installed with a fresh copy of debian-11 but we need to install some additional software and do updates. Install Updates First install all updates on the servers. To do this, run the following commands on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) apt update apt upgrade -y Set Timezone Set the correct timezone on all servers. Do this by running the following command on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) Replace values You have to replace YOUR_TIMEZONE with your timezone. timedatectl set-timezone YOUR_TIMEZONE #(1)! Replace YOUR_TIMEZONE with your timezone. Install packages To configure the hosts, we need to install some packages. To do this, run the following commands on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) apt install apparmor apparmor-utils -y #(1)! apparmor and apparmor-utils are needed as kernel security module.","title":"Servers"},{"location":"installation/servers/#servers-vms","text":"After creating the servers for the controlplane, we have to configure the operating system. The servers get installed with a fresh copy of debian-11 but we need to install some additional software and do updates.","title":"Servers (VMs)"},{"location":"installation/servers/#install-updates","text":"First install all updates on the servers. To do this, run the following commands on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) apt update apt upgrade -y","title":"Install Updates"},{"location":"installation/servers/#set-timezone","text":"Set the correct timezone on all servers. Do this by running the following command on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) Replace values You have to replace YOUR_TIMEZONE with your timezone. timedatectl set-timezone YOUR_TIMEZONE #(1)! Replace YOUR_TIMEZONE with your timezone.","title":"Set Timezone"},{"location":"installation/servers/#install-packages","text":"To configure the hosts, we need to install some packages. To do this, run the following commands on all servers: Warning You have to do this on all servers, created in the previous step. In this guide you will have to do this on 3 servers ( k8s-controlplane-hel1-1 , k8s-controlplane-fsn1-1 and k8s-controlplane-nbg1-1 ) apt install apparmor apparmor-utils -y #(1)! apparmor and apparmor-utils are needed as kernel security module.","title":"Install packages"},{"location":"prerequisites/","text":"Prerequisites This step will cover all the prerequisites you need to have in place before you can start deploying the cluster.","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"This step will cover all the prerequisites you need to have in place before you can start deploying the cluster.","title":"Prerequisites"},{"location":"prerequisites/container-registry/","text":"Container Registry To provide docker container images for the cluster you will need a container registry. In this example i will use the docker-hub but feel free to use other platforms like github-container-registry or something else. Create Account First, create an account at your container-registry provider. If you already have one, you can skip this step. If you want to use a docker-hub account, you can register here . Create Token To access the registry and push or pull images (pull only of you use private images) you will need a token. If you use the docker-hub, move to your security-profile-page for the token creation. The names of the tokens are not important, but you should know which token is for which purpose. Reminder Be shure to save the token in a save place because you need it later in the setup. The token is only shown once. You will need to create the following tokens: local-machine with write-access used for your local machine to push the created images If you want to use private images you also need to create a token for the cluster to pull the images: k8s-hetzner with read-access used on the kubernetes hosts to pull the images from the container registry","title":"Container Registry"},{"location":"prerequisites/container-registry/#container-registry","text":"To provide docker container images for the cluster you will need a container registry. In this example i will use the docker-hub but feel free to use other platforms like github-container-registry or something else.","title":"Container Registry"},{"location":"prerequisites/container-registry/#create-account","text":"First, create an account at your container-registry provider. If you already have one, you can skip this step. If you want to use a docker-hub account, you can register here .","title":"Create Account"},{"location":"prerequisites/container-registry/#create-token","text":"To access the registry and push or pull images (pull only of you use private images) you will need a token. If you use the docker-hub, move to your security-profile-page for the token creation. The names of the tokens are not important, but you should know which token is for which purpose. Reminder Be shure to save the token in a save place because you need it later in the setup. The token is only shown once. You will need to create the following tokens: local-machine with write-access used for your local machine to push the created images If you want to use private images you also need to create a token for the cluster to pull the images: k8s-hetzner with read-access used on the kubernetes hosts to pull the images from the container registry","title":"Create Token"},{"location":"prerequisites/dns-provider/","text":"DNS Provider To use ssl-certificates later, we will use the kubernetes cert-manager with lets-encrypt certificates. To use this, we need a dns provider for our domain that supports dns01-validation via acme . You can find supported dns providers in the documentation from the kubernetes cert-manager . In this tutorial i will use CloudFlare . Create Account First, you have to create an account at your dns-provider. If you want to use CloudFlare, you can create an account here . Setup Sites and DNS-Records After creating an account you have to add your domain as a new site to your dns provider. After that you can import your old dns entries or add new ones. As final step you need to change the nameservers from your domain. You can do this normally in the control panel from your domain hoster. Because these steps are different from dns provider to dns provider and different from domain hoster to domain hoster, I will skip this part in this tutorial. To follow the rest of the tutorial, you will later need the following dns entries: traefik.example.com -> A -> INGRESS_LOADBALANCER_PUBLIC_IP I will remind you to create the record in the correct step. Create Token To use the dns01-challenge, the acme client will create a txt dns-record for you to validate that you own the requested domain. To change the dns settings (add an entry) you have to create an access token for the acme client. If you use CloudFlare, move to your api-token-profile-page and create a new api-token. Dont use the global api token, you need a new api-token for your specific dns-zone. As token-template you can use the edit-dns-zone setting. In the next step you have to select your site you have created in the previous step . Reminder Remember to save the token, it will not be shown again.","title":"DNS Provider"},{"location":"prerequisites/dns-provider/#dns-provider","text":"To use ssl-certificates later, we will use the kubernetes cert-manager with lets-encrypt certificates. To use this, we need a dns provider for our domain that supports dns01-validation via acme . You can find supported dns providers in the documentation from the kubernetes cert-manager . In this tutorial i will use CloudFlare .","title":"DNS Provider"},{"location":"prerequisites/dns-provider/#create-account","text":"First, you have to create an account at your dns-provider. If you want to use CloudFlare, you can create an account here .","title":"Create Account"},{"location":"prerequisites/dns-provider/#setup-sites-and-dns-records","text":"After creating an account you have to add your domain as a new site to your dns provider. After that you can import your old dns entries or add new ones. As final step you need to change the nameservers from your domain. You can do this normally in the control panel from your domain hoster. Because these steps are different from dns provider to dns provider and different from domain hoster to domain hoster, I will skip this part in this tutorial. To follow the rest of the tutorial, you will later need the following dns entries: traefik.example.com -> A -> INGRESS_LOADBALANCER_PUBLIC_IP I will remind you to create the record in the correct step.","title":"Setup Sites and DNS-Records"},{"location":"prerequisites/dns-provider/#create-token","text":"To use the dns01-challenge, the acme client will create a txt dns-record for you to validate that you own the requested domain. To change the dns settings (add an entry) you have to create an access token for the acme client. If you use CloudFlare, move to your api-token-profile-page and create a new api-token. Dont use the global api token, you need a new api-token for your specific dns-zone. As token-template you can use the edit-dns-zone setting. In the next step you have to select your site you have created in the previous step . Reminder Remember to save the token, it will not be shown again.","title":"Create Token"},{"location":"prerequisites/hetzner/","text":"Hetzner We will use the hetzner cloud as cheap and easy cloud provider. To get everything working we need to create an account and a brandnew cloud project. The project needs new api-tokens and your ssh-key. We will cover the single steps in this section. Create Hetzner Account As first step you will need an account at hetzner. You can use my ref-link to get 20 EUR for free if you want. If you already have an account you can skip this step. Account Limits The hetzner cloud has limits for every resource you buy. You can request a scaleup for your account if you need more resources. You can do this on the limit overview page at the top right button. You will need to provide a reason for the scaleup. Remember that an autoscaling cluster will need at minimum the following resources: min. 4x Server (3x server, 1x agent) or more if you need more agents 2x LoadBalancer number of volumes you want to use number of snapshots you want to make Create Project The autoscaling cluster should sit in a plain cloud project . Login at console.hetzner.cloud/projects with your previously created account and create new project. You can name the project whatever you like. Create API-Token(s) The different applications and deployments will all need api-tokens to comminicate with the hetzner cloud. To create tokens open the project and go to security and the api-tokens tab. Here you have to create at least one api-token but i recommend to create one for each application to keep things separated. All tokens need read and write access. Reminder Save them in a secure place you will need them later in this guide and you cant view them another time inside the webpanel. I created the following tokens: command-line-interface (used for hcloud cli application on local machine) container-storage-interface (used for persistent volume driver) cloud-controller-manager (used for cloud-controller-manager) cluster-autoscaler (used for cluster autoscaler) Upload SSH-Key(s) Stay inside the security part of the hetzner webinterface and open the tab for the ssh-keys. Click add to upload your ssh-key(s). Paste your public key to the window. They will be later added to the servers (controlplane) when we create them. If you want to create a new ssh-key you can use ssh-keygen on your local machine. If you want, you can upload more than one ssh-key and add them all to the newly created servers.","title":"Hetzner"},{"location":"prerequisites/hetzner/#hetzner","text":"We will use the hetzner cloud as cheap and easy cloud provider. To get everything working we need to create an account and a brandnew cloud project. The project needs new api-tokens and your ssh-key. We will cover the single steps in this section.","title":"Hetzner"},{"location":"prerequisites/hetzner/#create-hetzner-account","text":"As first step you will need an account at hetzner. You can use my ref-link to get 20 EUR for free if you want. If you already have an account you can skip this step.","title":"Create Hetzner Account"},{"location":"prerequisites/hetzner/#account-limits","text":"The hetzner cloud has limits for every resource you buy. You can request a scaleup for your account if you need more resources. You can do this on the limit overview page at the top right button. You will need to provide a reason for the scaleup. Remember that an autoscaling cluster will need at minimum the following resources: min. 4x Server (3x server, 1x agent) or more if you need more agents 2x LoadBalancer number of volumes you want to use number of snapshots you want to make","title":"Account Limits"},{"location":"prerequisites/hetzner/#create-project","text":"The autoscaling cluster should sit in a plain cloud project . Login at console.hetzner.cloud/projects with your previously created account and create new project. You can name the project whatever you like.","title":"Create Project"},{"location":"prerequisites/hetzner/#create-api-tokens","text":"The different applications and deployments will all need api-tokens to comminicate with the hetzner cloud. To create tokens open the project and go to security and the api-tokens tab. Here you have to create at least one api-token but i recommend to create one for each application to keep things separated. All tokens need read and write access. Reminder Save them in a secure place you will need them later in this guide and you cant view them another time inside the webpanel. I created the following tokens: command-line-interface (used for hcloud cli application on local machine) container-storage-interface (used for persistent volume driver) cloud-controller-manager (used for cloud-controller-manager) cluster-autoscaler (used for cluster autoscaler)","title":"Create API-Token(s)"},{"location":"prerequisites/hetzner/#upload-ssh-keys","text":"Stay inside the security part of the hetzner webinterface and open the tab for the ssh-keys. Click add to upload your ssh-key(s). Paste your public key to the window. They will be later added to the servers (controlplane) when we create them. If you want to create a new ssh-key you can use ssh-keygen on your local machine. If you want, you can upload more than one ssh-key and add them all to the newly created servers.","title":"Upload SSH-Key(s)"},{"location":"prerequisites/local-machine/","text":"Local Machine In the last preparation step, we have to setup our local machine. As local machine you need a linux-host. You can install it directy to your host, use a virtual machine or - as i do - use wsl , the windows subsystem for linux. Packages To install the required packages, you can use the following commands: apt install apache2-utils -y #(1)! We need the apache2-utils package to generate the basic auth credentials for the traefik dashboard in a later step. hcloud To control the hetzner cloud from the command line you need hcloud, a command-line tool by hetzner. You can find more information here . Install hcloud You can install hcloud with homebrew . To install homebrew visit their website or run the following command: /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" As next step install hcloud to your system using homebrew: brew install hcloud Setup hcloud context To communicate with your hetzner account you created a cloud project in the hetzner prerequisite step . You have also added an api-token in a previous step . In my example the token named command-line-interface is relevant for the hcloud-cli. To link the cloud project with the hcloud application by using the api-token, you have to create a hcloud-context. You can manage different cloud-projects with different contexts. To create a new context type the following command and paste your api-token if it is asked: hcloud context create [ NAME ] # (1)! The name of the context. You can choose any name you want. It is recommended to use the name of the cloud project . You can see all contexts with hcloud context list and set your used context with hcloud context use [NAME] . Check that the hcloud-cli can connect to your cloud project by typing: hcloud server list You should get an empty list of servers, because we have not created any yet. Helm To install packages to kubernetes you need helm on your local machine. To install helm run the following commands. You can also visit the official installation manual . # Download latest helm install script curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 # Change access rights to execute the script chmod 700 get_helm.sh # run the script to install helm ./get_helm.sh Check if helm is installed correctly by running: helm version kubectl To administrate the kubernetes cluster from your local machine you also need kubectl, a command line interface to control kubernetes clusters. To install kubectl to your system, run the following commands or visit the kubernets documentation for installation steps: # Download latest kubectl release curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" # Download kubectl checksum file curl -LO \"https://dl.k8s.io/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl.sha256\" # Verify checksum file (output should be \"kubectl: OK\") echo \" $( cat kubectl.sha256 ) kubectl\" | sha256sum --check # Finally install kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check if kubectl is installed correctly by running: kubectl version --client go In the autoscaler step we need to build a custom docker image for the kubernetes cluster autoscaler. To build the image we need go. Install go to your local machine with the following commands. You can also view the official installation manual . # Remove old go installations rm -rf /usr/local/go && tar -C /usr/local -xzf go1.19.1.linux-amd64.tar.gz # Download latest go release wget https://go.dev/dl/go1.19.1.linux-amd64.tar.gz # Extract the archive sudo tar -C /usr/local -xzf go1.19.1.linux-amd64.tar.gz # Add go to the path export PATH = $PATH :/usr/local/go/bin # Remove downloaded archive rm go1.19.1.linux-amd64.tar.gz You can check the installation with: go version Docker You need docker on your local machine to build a docker image in the autoscaler step . Install Docker The installation of docker can be done in many different ways, i will show you the easiest way to install docker on your local machine using the following script: curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh You can also install docker using the package-manager or docker-desktop in wsl. You can find more installation steps in the docker documentation . You can check the installation with docker version . Login to Docker Hub You should login to your container registry created in a prerequisite step with the following command. Replace variables Replace DOCKER_USERNAME with your docker username and DOCKER_PASSWORD with your writeable token (In this guide i named the token local-machine ). docker login -u DOCKER-USERNAME -p DOCKER_PASSWORD #(1)! Your docker username and writeable token created in the container-registry step","title":"Local Machine"},{"location":"prerequisites/local-machine/#local-machine","text":"In the last preparation step, we have to setup our local machine. As local machine you need a linux-host. You can install it directy to your host, use a virtual machine or - as i do - use wsl , the windows subsystem for linux.","title":"Local Machine"},{"location":"prerequisites/local-machine/#packages","text":"To install the required packages, you can use the following commands: apt install apache2-utils -y #(1)! We need the apache2-utils package to generate the basic auth credentials for the traefik dashboard in a later step.","title":"Packages"},{"location":"prerequisites/local-machine/#hcloud","text":"To control the hetzner cloud from the command line you need hcloud, a command-line tool by hetzner. You can find more information here .","title":"hcloud"},{"location":"prerequisites/local-machine/#install-hcloud","text":"You can install hcloud with homebrew . To install homebrew visit their website or run the following command: /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" As next step install hcloud to your system using homebrew: brew install hcloud","title":"Install hcloud"},{"location":"prerequisites/local-machine/#setup-hcloud-context","text":"To communicate with your hetzner account you created a cloud project in the hetzner prerequisite step . You have also added an api-token in a previous step . In my example the token named command-line-interface is relevant for the hcloud-cli. To link the cloud project with the hcloud application by using the api-token, you have to create a hcloud-context. You can manage different cloud-projects with different contexts. To create a new context type the following command and paste your api-token if it is asked: hcloud context create [ NAME ] # (1)! The name of the context. You can choose any name you want. It is recommended to use the name of the cloud project . You can see all contexts with hcloud context list and set your used context with hcloud context use [NAME] . Check that the hcloud-cli can connect to your cloud project by typing: hcloud server list You should get an empty list of servers, because we have not created any yet.","title":"Setup hcloud context"},{"location":"prerequisites/local-machine/#helm","text":"To install packages to kubernetes you need helm on your local machine. To install helm run the following commands. You can also visit the official installation manual . # Download latest helm install script curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 # Change access rights to execute the script chmod 700 get_helm.sh # run the script to install helm ./get_helm.sh Check if helm is installed correctly by running: helm version","title":"Helm"},{"location":"prerequisites/local-machine/#kubectl","text":"To administrate the kubernetes cluster from your local machine you also need kubectl, a command line interface to control kubernetes clusters. To install kubectl to your system, run the following commands or visit the kubernets documentation for installation steps: # Download latest kubectl release curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" # Download kubectl checksum file curl -LO \"https://dl.k8s.io/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl.sha256\" # Verify checksum file (output should be \"kubectl: OK\") echo \" $( cat kubectl.sha256 ) kubectl\" | sha256sum --check # Finally install kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check if kubectl is installed correctly by running: kubectl version --client","title":"kubectl"},{"location":"prerequisites/local-machine/#go","text":"In the autoscaler step we need to build a custom docker image for the kubernetes cluster autoscaler. To build the image we need go. Install go to your local machine with the following commands. You can also view the official installation manual . # Remove old go installations rm -rf /usr/local/go && tar -C /usr/local -xzf go1.19.1.linux-amd64.tar.gz # Download latest go release wget https://go.dev/dl/go1.19.1.linux-amd64.tar.gz # Extract the archive sudo tar -C /usr/local -xzf go1.19.1.linux-amd64.tar.gz # Add go to the path export PATH = $PATH :/usr/local/go/bin # Remove downloaded archive rm go1.19.1.linux-amd64.tar.gz You can check the installation with: go version","title":"go"},{"location":"prerequisites/local-machine/#docker","text":"You need docker on your local machine to build a docker image in the autoscaler step .","title":"Docker"},{"location":"prerequisites/local-machine/#install-docker","text":"The installation of docker can be done in many different ways, i will show you the easiest way to install docker on your local machine using the following script: curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh You can also install docker using the package-manager or docker-desktop in wsl. You can find more installation steps in the docker documentation . You can check the installation with docker version .","title":"Install Docker"},{"location":"prerequisites/local-machine/#login-to-docker-hub","text":"You should login to your container registry created in a prerequisite step with the following command. Replace variables Replace DOCKER_USERNAME with your docker username and DOCKER_PASSWORD with your writeable token (In this guide i named the token local-machine ). docker login -u DOCKER-USERNAME -p DOCKER_PASSWORD #(1)! Your docker username and writeable token created in the container-registry step","title":"Login to Docker Hub"}]}